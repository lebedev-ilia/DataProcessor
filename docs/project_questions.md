# Вопросы по проекту TrendFlow / DataProcessor (для фиксации контрактов)

Этот файл — список вопросов, которые нужно прояснить, чтобы стабилизировать архитектуру, контракты данных и процесс разработки.  
После ответов мы будем фиксировать решения в документации (контракты/правила) и рефакторить компоненты.

---

## 1) Цели продукта и что именно предсказываем

- **целевая платформа**: это YouTube / TikTok / Shorts / Instagram Reels / “все сразу”? Отличается ли логика метрик и доступность данных?
- **целевые таргеты**: что именно предсказываем?
  - просмотры/лайки/комменты через 1 месяц
  - скорость роста (derivatives)
  - вероятность “вирусности” (классификация)
  - retention/CTR (если есть)
- **мульти-таргет**: будет 1 модель на всё или отдельные модели под разные таргеты?
- **нормализация таргетов**: таргет абсолютный или нормированный (на подписчиков канала / baseline канала / категорию)?
- **горизонт прогнозирования**: 7 дней / 30 дней / 90 дней? один горизонт или несколько?
- **пользовательский кейс**: кто потребитель предсказания?
  - автор (подсказки как улучшить)
  - аналитик (интерпретируемость)
  - внутренняя система ранжирования/отбора
- **метрики качества модели**: какие KPI считаем успехом? (MAE/RMSE/R², rank metrics NDCG, calibration, uplift)

Ответы:

**целевая платформа** - На данный момент были собраны данные только с YouTube так как там их легче всего собрать через их API, но по факту на других платформах данные будут идентичны просто собирать их сложнее и я еще не писал алгоритмы под парсинг данных например под instagram или TikTok. То есть сейчас у меня данные по 100к видео собраных с YouTube для обучения модели но после ее обучения и в дальнейшем использование в продакшене можно применять данные с других платформ используя новые алгоритмы и предобработку.

**целевые таргеты** - На данные момент я планировал предсказывать только просмотры или/и лайки на месяц вперед, но возможно ты поможешь мне усовершенствовать систему для предсказание чего либо еще без масштабной доработки.

**мульти-таргет** - По скольку пока мы ориентируемся на один таргет - просмотры через месяц то и модель будет одна. Но поскольку я не проффесионал то не знаю, может эта одна модель может сразу предсказывать и просмотры и лайки.

**нормализация таргетов** - Не совсем понимаю что это значит

**горизонт прогнозирования** - На данный момент рассматриваеться только предсказания на 30 дней вперед

**пользовательский кейс** - Система предназначена как для автора видео, который просто хочет узнать прогнозы своего видео и получить базовый анализ и рекомендации, так и для проффесиональных аналитиков, которые помимо простого анализа и предсказания хотят получать подробный разбор и анализ, интерпретации.

**метрики качества модели** - Тут я точно не знаю, надеюсь ты мне поскажешь что будет лучше в моем случае.

Рекомендации / комментарии (ChatGPT):

**Платформа**
- YouTube как MVP — отлично. Но “другие платформы идентичны” — не совсем так: механики платформ отличаются, поэтому переносимость модели без адаптации не гарантируется.
- Полуфинальное правило: в данных и в `meta` всех артефактов всегда есть `platform_id` (например `"youtube"`).

**Таргеты**
- Можно (и обычно полезно) обучать одну модель с несколькими “головами” (multi-head): `views_30d` и `likes_30d`.
- Полуфинальное правило: таргеты считаем и оцениваем в log-scale (`log1p(views_30d)`, `log1p(likes_30d)`).

**Что такое нормализация таргетов**
- Если предсказывать “сырые просмотры”, модель часто учит “размер канала” вместо качества контента.
- Варианты нормализации (выбираем 1–2 для MVP):
  - абсолютный таргет: `log1p(views_30d)`
  - относительный: `log1p(views_30d / (subscriberCount_0 + 1))`
  - uplift к baseline канала (если есть история): `log1p(views_30d / (baseline + 1))`

**Метрики качества**
- Регрессия: MAE/RMSE на `log1p`.
- Ранжирование: Spearman (и/или NDCG по группам).

Новые вопросы (ответь ниже):
1) Что важнее показывать пользователю: **абсолютный прогноз** (views30) или **uplift относительно канала**?
2) Ок ли multi-target (views+likes) в одной модели на MVP?
3) Планируешь ли multi-horizon (7/14/21/30) в одной модели, или строго 30 дней?

Ответы: 

1. абсолютный прогноз
2. multi-target - супер
3. multi-horizon - да будет супер

Комментарий к ответам (ChatGPT, раунд 2):

**Фиксируем решения**
- UI показывает **абсолютный прогноз** (views/likes). При этом мы можем дополнительно считать “relative/uplift” как internal metric/feature — это часто повышает качество, даже если не показываем пользователю.
- Multi-target (views+likes) = да.

**Важно про multi-horizon**
- Чтобы это было формально, надо зафиксировать реальные горизонты в данных. Ты писал “4 снапшота с шагом неделя” → это обычно горизонты **7/14/21 дней**, а не 30.
- Полуфинальное правило: multi-horizon targets считаем как **дельты** (приросты), а не абсолюты:
  - `delta_views_h = viewCount_t+h - viewCount_t`
  - аналогично для likes/comments/subs

Новый вопрос (ответь ниже):
1) Снапшоты сняты на горизонтах 0/7/14/21 дней или есть ещё 30-й день (0/7/14/30)?

Комментарий к ответу (ChatGPT, раунд 3):

Похоже ответ оборвался — допиши, пожалуйста, что именно за “нюанс” (это критично для корректного multi-horizon).

Чтобы мы могли формально зафиксировать правило, нужны ровно 2 вещи:
1) Есть ли у каждого снапшота **timestamp сбора** (например `snapshot_collected_at` / `fetched_at`), чтобы считать реальные \(\Delta t\) в днях?
2) Всегда ли интервалы одинаковые, или могут быть “плавающие” (например 6–10 дней)?

Если timestamps есть — предлагаю полуфинальный контракт:
- Горизонты считаем по реальному времени: `h_days = (t_k - t_0).days`
- Таргеты считаем как дельты: `delta = metric(t_k) - metric(t_0)`
- Модель предсказывает набор горизонтов, который реально присутствует в данных (обычно 3 головы для 7/14/21).

Новые вопросы (ответь ниже):
1) Где у тебя хранится время сбора снапшота (какое поле/файл)?
2) Интервалы между снапшотами строго 7 дней или плавают?

Ответы:

1. Крч я закосячил 2 снапшот и много видео не собралось (примерно 20тыс.), то есть мета-снапшот сделан хорошо на 100к, второй снапшот (через неделю) - 70к, а остальные нормально по 100к, грубо говоря есть 2 полных снапшота - через 2 недели от первого и через 3 недели от первого. Как я понимаю это не критично просто теперь нужно четко зафиксировать что модель должна предсказывать
2. время сбора снапшота храниться в самой папке снапшота в датасете (HuggingFace)
3. Интервалы между сбором данных по конкретному видео могут отличаться максимум на пару часов но таких видео не так много, в основном разница меньше часа, грубо говоря почти ровно неделя

Комментарий к ответам (ChatGPT, раунд 4):

**Важно: у тебя ответ “3.”, хотя вопрос был (1) и (2)**
- Это ок — я трактую так:
  - (1) = проблема со snapshot2 (7d) и missing labels
  - (2) = timestamps лежат в папке снапшота + интервалы почти ровно неделя

**Фиксируем multi-horizon схему под твой реальный датасет**
- Основные горизонты для всей выборки: **14d и 21d** (потому что они полные на ~100k).
- Горизонт 7d — **частичный** (есть только на ~70k) → он остаётся как optional head с mask.

Полуфинальное правило (training targets):
- Определяем `t0 = snapshot1_time`.
- Если есть snapshot на `t14`, то:
  - `y_views_14 = log1p(viewCount(t14) - viewCount(t0))`
  - `y_likes_14 = log1p(likeCount(t14) - likeCount(t0))`
- Аналогично для 21d.
- Для 7d: считаем только там, где snapshot действительно есть, иначе ставим missing и выключаем loss по этому head.

Полуфинальное правило (качество данных):
- Для каждого горизонта делаем маску `has_horizon_7/14/21`.
- В метриках всегда показываем качество отдельно по head’ам (7/14/21).

---

## 2) Данные (Fetcher) и качество датасета

- **источник данных**: как собираются 100k видео? есть ли лицензии/ограничения?
- **what is a “snapshot”**: что точно входит в 4 снапшота?
  - мета (title/desc/tags/channel)
  - комментарии (все/топ/последние)
  - временные метрики (likes/views/comments/subs)
- **какие поля меняются**: что считаем динамикой, а что “статикой”?
- **пропуски данных**: что делаем, если один из снапшотов не собрался / часть комментов недоступна?
- **дубликаты**: как определяется уникальность видео? (reupload, same content, different id)
- **язык**: язык известен или нужно детектить? бывает ли мультиязычность (title RU, comments EN)?
- **каналы**: учитываем ли историю канала? (например, средние просмотры по каналу)
- **аудио/видео доступ**: всегда ли есть возможность скачать исходный ролик? качество (720p/1080p), частота кадров, длительность?
- **баланс датасета**: распределение по нишам/жанрам/языкам/длинам видео?
- **этика/PII**: есть ли ограничения по хранению лиц/текста/комментариев?

Ответы:

**источник данных - Данные уже собраны. Лицензий нет. Нужно подробнее объяснить мне эту тему

**what is a “snapshot” - В metadata входят: 

title - заголовок видео
description - описание видео (может не быть)
tags - тэги из заголовка и описания (может не быть)
language - язык 
madeForKids - предназначено ли видео для детей
thumbnails - словарь в котором есть ссылка на thumbnail, его высота и ширина
publishedAt - дата и время публикации
channelTitle - название канала, с которого было опуьликовано видео
country - стнана публикации
webpage_url - ссылка на видео
age_limit - возрастное ограничение
subtitles - субтитры автора (может не быть)
automatic_captions - автоматические субтитры YouTube (могут быть en, ru или и то и то) (может не быть)
chapters - автор мог разделить видео на части и тут указаны временные отрезки (может не быть)
formats - форматы для скачивания, содержит 2 лучших формата. В каждом есть разрешение, fps и кодеки.
thumbnails_ytdlp - словарь в котором есть ссылка на thumbnail, его высота и ширина (взято из yt_dlp так как youtube может не отдать thumbnails)
duration_seconds - продолжительность видео (ограничения во время сбора - от 4 сек до 1200 сек)

В каждом из 4 снапшотов (кол-во на момент сбора):

viewCount
likeCount
commentCount
subscriberCount
comments - список коментариев, в кажом коментарии есть кол-во лайков и ответов на него, текст, автор, дата публикации. По каждому видео береться только 100 самых релевантных комментариев. Релевантность определяет сам YouTube. (Может не быть)
videoCount - кол-во видео канала
viewCount_channel - общее кол-во просмотров на канале со всех видео

**какие поля меняются - metadata это статика, она не меняеться со временем. То что в снапшотах - меняеться.

**пропуски данных - честно не знаю как это грамотно обрабатывать. Твоя совет будет уместен

**дубликаты - у YouTube есть video_id. На этапе сборки и фильтрации дубликаты много раз исключались но в итоговом датасете могут все еще быть (менее 1%)

**язык - Да, мультиязычность может быть, например видео выпустил русскоязычный автор из россии, но комментарии могут быть на  другом языке или наоборот.

**каналы - Да, эти данные есть (subscriberCount, videoCount, viewCount_channel), но я пока не думал как их правильно обрабатывать. Нужна твоя помощь

**аудио/видео доступ - Видео уже скачаны в хранилище (100к видео). Распределения длительности пока нет но можно построить так как данные уже есть

**баланс датасета - На этапе сбора применялась динамическа фильтрация по лайкам, просмотрам, длительности, поэтому датасет относительно сбалансированный. Но всеравно нужно построить все расспределения и графики что бы убедиться.

**этика/PII - Тут нужны твои советы.

Рекомендации / комментарии (ChatGPT):

**Лицензии / ToS / PII**
- Я не юрист, но инженерно лучше заранее выбрать политику хранения:
  - “research” режим может хранить raw видео/комменты,
  - “prod” режим по возможности хранит только derived features/эмбеддинги/агрегаты, а raw — ограниченно и временно.
- Полуфинальное правило: source-of-truth для ML и повторных прогонов = NPZ-фичи, а не raw данные.

**Пропуски**
- Полуфинальное правило: пропуски не заменяем нулями. Используем `NaN` + маски `has_*` / `*_present`.

**Комментарии**
- Топ-100 “релевантных” комментариев = bias (YouTube ранжирует) — это нормально, просто фиксируем в документации.
- Очень желательно иметь стабильные `comment_id` и `channel_id`, чтобы:
  - кэшировать между снапшотами,
  - честно считать динамику.

**Канальные фичи (быстрый gain)**
- derived:
  - `views_per_video_channel = viewCount_channel / max(videoCount,1)`
  - `views_per_subscriber = viewCount / max(subscriberCount,1)`
  - `like_rate = likeCount / max(viewCount,1)`

Новые вопросы (ответь ниже):
1) У тебя есть `channel_id` и `comment_id` (стабильные идентификаторы), или только channelTitle/author?
2) Snapshot-1 собирается ровно “в момент публикации” или может быть заметно позже?
3) Готов ли ты в продакшене хранить не raw текст комментариев, а только агрегаты/эмбеддинги (privacy/ToS)?

Ответы: 

1. Для видео есть только channelTitle. Для комментариев только authorName
2. Тут была такая стратегия: я выделил 8 временных отрезков - less-1day, 1day-1week, 1week-1month, 1month-3month, 3month-6month, 6month-1year, 1year-3year, 3year-more. Вот примерное расспределдение видео:
TIME_INTERVALS_NUM_VIDEOS = {
    "less-1day": 900, 
    "1day-1week": 1130, 
    "1week-1month": 1400, 
    "1month-3month": 1640, 
    "3month-6month": 1250, 
    "6month-1year": 1080, 
    "1year-3year": 950, 
    "3year-more": 700
}. Это расспределение видео для одной категории, например Спорт. У меня 18 категорий и в сумме получаеться более 100к видео, но в ходе сбора много ошибок и фильтраций и в итоге получилось около 100к. Как видно Большинство видео было собрано с временем публикации в диапозоне от 1 месяця до 3 месяцев.
3. Да, думаю что можно один раз брать тексты, извлекать все что нужно и далее не хранить raw.

Комментарий к ответам (ChatGPT, раунд 2):

**Нет `channel_id` / `comment_id` — это риск**
- `channelTitle` и `authorName` не являются стабильными идентификаторами (могут совпадать, меняться).
- Это бьёт по:
  - честному split “по каналам” (защита от leakage)
  - кэшированию/динамике комментариев между снапшотами

Полуфинальное правило (предлагаю):
- Добавляем этап “dataset enrichment”: по `video_id` дотягиваем **channel_id** (и по возможности `comment_id`/`authorChannelId`) и сохраняем в датасете.
- Если API/данные не позволяют — фиксируем ограничение, но тогда split по каналам будет “примерным”.

**Про Snapshot-1**
- Из твоего распределения видно, что Snapshot-1 часто снят через 1–3 месяца после публикации. Это значит, что продукт “предсказать успех в момент публикации” этим датасетом напрямую не покрывается.
- Полуфинальное правило: MVP прогноз = “предсказать прирост метрик в следующие N дней **от текущего момента** (snapshot1 time)”.

Новые вопросы (ответь ниже):
1) Ты готов сделать enrichment-скрипт, который по `video_id` добавит `channel_id` в meta? (Да/Нет)
2) MVP-кейс “пользователь даёт ссылку на уже опубликованное видео → прогноз на следующий месяц” — это ок как главный MVP?

Подсказка (чтобы было проще ответить):
- Если ты хочешь MVP “прогноз для уже опубликованного видео” — это отлично ложится на твой датасет (snapshot1 часто не в момент публикации).
- Если ты хочешь MVP “прогноз до публикации/в момент публикации” — нужно менять сбор данных (snapshot1 должен быть близок к публикации).

Ответы:

1. Да я готов сделать
2. Ну смотри, у нас процент собраных видео с временем публикации меньше одного дня примерно 10-13% от всего датасета, так что я думаю модель сможет более менее нормально предсказывать на видео в момент публикации или почти в момент публикации. Ну а так да, я готов к тому что лучшее качество предсказания будет на видео которое уже какое то время находиться в публичном доступе

Комментарий к ответам (ChatGPT, раунд 3):

Ок, тогда фиксируем MVP как “универсальный”, но с явным учётом возраста видео.

Полуфинальное правило:
- Во входные фичи Temporal обязательно добавляем `video_age_hours_at_snapshot1 = snapshot1_time - publishedAt`.
- При обучении и отчётах делаем стратификацию по возрасту:
  - cold-start: <24h (твои 10–13%)
  - warm-start: 1–30d
  - older: >30d

Так мы получаем честные ожидания качества: модель может работать и “в момент публикации”, но качество будет ниже, и мы это измеряем/показываем.

Новый вопрос (ответь ниже):
1) Ты хочешь один универсальный baseline+model с фичей `video_age_hours`, или отдельную “cold-start” модель для <24h?

Комментарий (ChatGPT):

Моя рекомендация для MVP (качество + простота):
- **Одна универсальная модель** + явная фича `video_age_hours_at_snapshot1` + стратифицированные метрики по возрастным корзинам.

Почему так лучше на старте:
- Данных <24h у тебя ~10–13% → отдельная модель будет менее стабильна и сильнее переобучится.
- Универсальная модель сможет учить общий сигнал и “понимать”, что в cold-start меньше наблюдаемой динамики.

План эволюции (если понадобится):
- Позже можно добавить “режим”/head для cold-start или второй моделью только когда увидим, что качество <24h слишком плохое и есть смысл специализироваться.

Новый вопрос (ответь ниже):
1) Ок ли фиксировать MVP как **одна модель** (universal) и отдельно мониторить качество на <24h?

Ответы:

1. Да одна универсальная модель

Комментарий к ответам (ChatGPT, раунд 6):

**Фиксируем как полуфинал**
- MVP = **одна универсальная модель**.
- `video_age_hours_at_snapshot1` = обязательная фича (и для baseline, и для любых seq-моделей).
- Отчётность по качеству всегда включает разрез по возрасту:
  - `<24h`, `1–30d`, `>30d` (при необходимости добавим более “старые” корзины).

**Правило для обучения/валидации**
- Метрики считаем:
  - overall
  - и по age-buckets (чтобы не “улучшить overall” ценой провала cold-start).
---

## 3) Общая архитектура ML: Transformers + Fusion

- **статус плана**: это окончательный дизайн или “черновой”?
- **входы трансформеров**:
  - VisualTransformer получает что именно: embeddings per-frame/per-shot/per-segment?
  - AudioTransformer: какие признаки/частота/окна?
  - TemporalTransformer: какие временные метрики и с какой дискретизацией?
- **длины последовательностей**: какой target length на вход каждого трансформера? (max_len, padding, masking)
- **агрегации для Fusion**: какие агрегаты ты точно хочешь иметь, а какие — опционально?
- **shared embedding space**: нужно ли приводить Audio/Visual/Text в общее пространство (projection heads)?
- **обучение**: end-to-end или:
  - сначала frozen-энкодеры + обучение Fusion
  - потом fine-tune энкодеры
- **какие “metadata” в Fusion**: channel stats, язык, категория, длина, время публикации, etc?
- **что идёт из TextProcessor**: только агрегаты, или последовательность тоже возможна?

Ответы:

**статус плана - черновой. Все еще можно переделать, изменить, но данные уже собраны.

**входы трансформеров**
**длины последовательностей**
**агрегации для Fusion**
**shared embedding space**
**обучение**
**какие “metadata” в Fusion**
**что идёт из TextProcessor** - На все эти вопросы мне нужны твои проффесиональные советы, так все еще можно изменить (кроме собраных данных), то лучше это сделать с тобой вместе что бы прийти к хорошему качеству.

Рекомендации / комментарии (ChatGPT):

Чтобы быстрее прийти к качеству и не “утонуть” в архитектуре, предлагаю 2 этапа:

**Этап A (baseline)**
- Сделать сильный baseline на табличных фичах (CatBoost/LightGBM):
  - meta + snapshot1 stats + channel stats
  - агрегаты Visual/Audio/Text
- Цель: понять вклад блоков фичей, прежде чем строить сложные transformers.

**Этап B (sequence + transformers)**
- После baseline зафиксировать контракты:
  - max_len (например 256/512/1024)
  - что такое “токен” (frame/shot/scene)
  - padding/mask policy

Полуфинальное правило:
- “Сначала baseline → потом transformers. Transformers не блокируют MVP.”

Новые вопросы (ответь ниже):
1) У тебя есть ресурсы на регулярное обучение трансформеров (GPU постоянно/часто), или ограниченно?
2) Для первого MVP важнее “качество любой ценой” или быстрые итерации (скорость разработки/экспериментов)?

Ответы:

1. ресурсы есть
2. качество

Комментарий к ответам (ChatGPT, раунд 2):

Ок, раз приоритет “качество”, то план такой:
- baseline всё равно делаем первым (это не про “скорость”, это про быстрый контроль качества данных + sanity-check).
- затем подключаем transformers/sequence, но только после того, как зафиксированы контракты артефактов.

Полуфинальное правило:
- baseline-модель является обязательной “контрольной точкой” для каждого большого изменения фичей/схем.

Комментарий / рекомендации (ChatGPT, раунд 3 — конкретизируем Transformers/Fusion):

Так как у нас большой разброс по длинам видео и мы уже вводим `cut_detection`, лучший “unit token” почти всегда:
- **token = shot** (а не каждый кадр)

Почему:
- shots дают естественное “сжатие” длинных видео без потери структуры (пейсинг/монтаж/сюжет).
- для коротких видео shots тоже работают (просто токенов мало).

Полуфинальный контракт (sequence outputs):
- VisualProcessor должен уметь выдавать **per-shot последовательность** (для трансформера) + **агрегаты** (для baseline/fusion).
- Пер-кадровые последовательности допускаются только как internal/debug и не идут в training schema по умолчанию.

Полуфинальный дизайн v1 (простая и сильная база):
- **Visual tokens**: берём `core_clip` embeddings на sampled frames, затем **pool per-shot** (mean/attention pooling) → получаем `E_shot` (shape `[n_shots, d_clip]`).
- **Audio tokens**: считаем CLAP/аудио‑эмбеддинги на тайм‑окнах (например 1–2s) и тоже **pool per-shot по времени** → `A_shot`.
- **Temporal/meta**: скаляры (длина, категория, язык, age, channel stats) → MLP → добавляем как отдельный `meta_token` или через FiLM/conditioning.
- **Fusion**: на старте достаточно:
  - либо один Transformer над concatenated tokens `[E_shot || A_shot || extra_shot_features]`
  - либо “late fusion”: отдельные энкодеры + concat pooled outputs + MLP head.

Max sequence length (чтобы не взорвать память):
- Рекомендую зафиксировать `max_len_shots = 256` для MVP.
- Если `n_shots > 256` → downsample (например merge соседних shots по времени или top‑K “наиболее информативных” по дешёвым сигналам).

Обучение (стабильность):
- MVP: энкодеры (CLIP/CLAP) frozen, учим projection+transformer+heads.
- Fine-tune позже, когда schema стабилизируется и мы уверены, что данные/семплинг корректные.

Новые вопросы (ответь ниже):
1) Подтверждаешь, что для VisualTransformer **token=shot** — основной формат sequence?
2) Ок ли `max_len_shots = 256` как MVP‑стандарт?
3) Хочешь “late fusion” (проще/стабильнее) как v1, а “единый multimodal transformer” как v2?

Ответы:

1. Да
2. Да
3. Да, но в продакш при старте проекта пойдет v2 (multimodal transformer)

Комментарий к ответам (ChatGPT, раунд 4):

**Фиксируем**
- Sequence token = **shot**.
- `max_len_shots = 256` = MVP стандарт.
- В проде на старте можно запускать **v2 (multimodal transformer)** — ок.

**Но критично: “v1” всё равно нужен как контроль**
Чтобы прод v2 не стал “чёрным ящиком без тормозов”, предлагаю полуфинальный процесс:
- v0: baseline (CatBoost/LightGBM) — обязателен.
- v1: late-fusion (простая архитектура) — как **sanity-check** и резервный вариант.
- v2: multimodal transformer — основной прод‑вариант.

Полуфинальное правило:
- Любое большое изменение фичей/семплинга сначала проходит через baseline + v1 (быстро проверить, что данные не “сломались”),
- после этого обучаем v2 и только затем выкатываем.

**Контракт v2 (MVP‑версия, без избыточной сложности)**
- Вход: `shot_tokens` длиной `<=256`.
- `E_shot` (visual) и `A_shot` (audio) приводим к общему `d_model` через projection.
- Missing modality допустима: используем `*_present` masks и/или learnable “missing token”.
- Добавляем `meta_token` (скаляры: возраст, длительность, категория, язык, channel stats).
- Выходы: multi-target (views/likes) × multi-horizon (7/14/21, где 7 masked).

**Производство / воспроизводимость**
- В проде фиксируем `model_version` + `training_data_version` + `feature_schema_versions` (ссылки на schema_version NPZ).
- Если v2 недоступен/в ошибке → можно деградировать на baseline/v1 (как “safety net”).
---

## 4) DataProcessor как продукт: вход/выход, версии, совместимость

- **что является входом DataProcessor**:
  - raw video file + meta + comments?
  - уже нарезанные кадры/аудио?
  - ссылка на видео?
- **что является единицей обработки**:
  - 1 видео → 1 папка артефактов
  - 1 видео → несколько версий артефактов (по разным конфигам)
- **версионирование артефактов**:
  - нужен ли глобальный `dataprocessor_version` для совместимости моделей?
- **стабильность контрактов**:
  - можно ли менять ключи/формы в NPZ без миграции?
  - нужен ли “schema registry” для артефактов?

Ответы:

**что является входом DataProcessor** - raw video file + meta + comments

**что является единицей обработки** - Хороший вопрос на который мне также нужно твое проф. мнение. Для обучения моделей я хочу выжать по максимуму из обработки и фичей, то есть будет один максимальный конфиг, НО потом в продакшене я бы хотел сделать многие алгоримы опциональными, то есть пользователь при настройке анализа может выбрать какие выходыне данные ему нужны, например если он точно хочет видеть предсказания значит у него будут обязательные поля, на которых обучалась модель, но в дополнении DataProcessor может извлечь ему дополнительные данные для анализа или рекомендаций, которые не пойдут конкретно в модели.

**версионирование артефактов** и **стабильность контрактов** - Тут нужен твой совет. Что говорят лучшие практики и советы? Как лучше сделать для продакшена?

Рекомендации / комментарии (ChatGPT):

Полуфинальные правила (предлагаю):

**Run identity**
- `platform_id + video_id + run_id + config_hash` = уникальный прогон.
- В каждом NPZ в `meta` всегда: `platform_id`, `video_id`, `run_id`, `config_hash`.

**Опциональность модулей**
- Для обучения модели нужен фиксированный “training schema” (обязательный набор модулей/фичей).
- В проде можно считать дополнительные фичи опционально, но обязательный набор должен совпадать с обучением.

**Версии**
- В каждом NPZ: `producer_version` и `schema_version`.
- Любое изменение схемы = bump `schema_version` + запись в changelog (без “тихих” изменений).

Новые вопросы (ответь ниже):
1) Ты хочешь структуру хранения артефактов per-video/per-run: `result_store/<platform>/<video_id>/<run_id>/...`?
2) `run_id` генерирует backend (по запросу пользователя) или DataProcessor (внутри)?

Ответы:

1. Нужен твой совет, но я думаю что лучше делать per-run так как так модулям мудет легче понимать что им обрабатывать и куда это класть, но при этом если будут запросы на видео которые уже были обработаны мы должны хранить кэш, например последние 10000 тыс. обработаных проектом видео
2. Сделаем как ты считаешь правильным

Комментарий к ответам (ChatGPT, раунд 2):

**Per-run storage — подтверждаю, это правильно**
- Мы фиксируем per-run директории, чтобы параллельные прогоны (разные конфиги/версии) не конфликтовали.

**Как совместить per-run и кэш**
- Кэш — это не “не делать per-run”, а “уметь переиспользовать уже готовый run”.
- Полуфинальное правило: вводим индекс артефактов (manifest/DB) по ключу:
  - `(platform_id, video_id, config_hash, sampling_policy_version, dataprocessor_version)`
  - значение: `latest_success_run_id` (+ ссылка на артефакты)

**`run_id` (рекомендация)**
- Для продакшена лучше: `run_id` генерирует backend (или orchestrator верхнего уровня) и передаёт в DataProcessor.
- Для offline/batch: DataProcessor может генерировать `run_id` сам.

Новые вопросы (ответь ниже):
1) Ок ли тебе идея “artifact index” (manifest/таблица), чтобы по ключу быстро находить уже посчитанные результаты?
2) Кэш “последние 10k видео”: это про **количество уникальных video_id** или про **количество run-ов**? (важно для стоимости)

Ответы:

1. Да ок
2. 10000 это значения на вскидку. Это про кол-во уникальных видео. Храним их для такого случая например: вышло какое классное видео которое точно посмотрит много людей и вот первый человек просит его анализ, при этом мы уверены что анализ по этому видео точно в ближайшее время могут запросить многи другие люди и для них мы уже можем взять тот результат первого человека и немного адаптировать под конкретных людей без глобального пересчета.

Комментарий к ответам (ChatGPT, раунд 3):

Классная идея — главное формализовать “что значит адаптировать под конкретных людей”, чтобы кэш работал корректно.

Полуфинальное правило (2 слоя результата):
- **Heavy compute (video-level)**: core providers + Visual/Audio фичи считаются один раз и кэшируются по ключу (video_id + sampling + versions).
- **Light personalization (user-level)**: “рендер” результата (JSON/советы/скоринг/выбор языка) делается быстро поверх уже готовых NPZ и может зависеть от пользователя.

Новый вопрос (ответь ниже):
1) Что именно ты имеешь в виду под “адаптировать под конкретных людей”: язык UI, набор модулей, уровень детализации, или персональные предпочтения автора?

Комментарий (ChatGPT):

Чтобы кэш был корректным и не было “магии”, предлагаю зафиксировать:

Полуфинальное правило (что можно “адаптировать” без пересчёта тяжёлых фич):
- язык UI/отчёта (RU/EN)
- уровень детализации (short summary vs detailed)
- формат выдачи (frontend cards vs analyst export)
- персональные советы/шаблоны рекомендаций (LLM/prompting) поверх фиксированных метрик

Полуфинальное правило (что НЕЛЬЗЯ адаптировать без пересчёта):
- сами Video/Audio/Visual NPZ фичи, если меняются модели/семплинг/версии
- любые “персональные” фичи, которые требуют доп. вычислений на сигнале (это уже новый run)

Новые вопросы (ответь ниже):
1) Под “адаптацией” ты имел в виду именно **presentation layer** (язык/формат/советы), без пересчёта фичей — верно?
2) Ты хочешь персонализацию рекомендаций через LLM (текстовые советы) как часть продукта MVP?

Ответы:

1. Да
2. Да

Комментарий к ответам (ChatGPT, раунд 4):

**Фиксируем как полуфинал: LLM = только presentation layer**
- LLM не добавляет “новые video-level фичи” и не требует пересчёта Visual/Audio NPZ.
- LLM читает агрегаты/метаданные/selected highlights из NPZ и рендерит:
  - summary
  - рекомендации
  - объяснения (почему так)

**Воспроизводимость (важно для дебага и качества)**
- Любой LLM-рендер должен иметь версию:
  - `llm_provider` (openai/…)
  - `llm_model` (например `"gpt-4.1-mini"`, потом уточним)
  - `prompt_version`
  - `prompt_hash`
- Это пишем либо в `manifest.json`, либо в отдельный `render.json` рядом с результатом пользователя.

**Кэширование**
- Heavy compute кэшируем по видео (как и договорились).
- LLM-рендер кэшируем отдельно по ключу:
  - `(video_id, run_id, render_profile_id, llm_model, prompt_hash, locale)`
  - чтобы повторные запросы были дешёвыми и одинаковыми.

**Guardrails (чтобы не было юридических/продуктовых сюрпризов)**
- LLM не должен “выдумывать метрики”: любые числа/факты берём только из NPZ.
- Если данных нет (NaN/missing) — LLM обязан явно сказать “данные недоступны”.

Новые вопросы (ответь ниже):
1) Хочешь, чтобы LLM генерировал **только текст**, или ещё и **структурированный JSON** (например `advice_cards[]`) для фронта?
2) Нужен ли “профиль автора” (например ниша/стиль/цель канала) как вход в LLM‑рендер, или пока без этого?

Ответы:

1. Только текст
2. Пока без этого

Комментарий к ответам (ChatGPT, раунд 5):

**Фиксируем (MVP)**
- LLM генерирует **только текст**.
- “Профиль автора” как вход пока **не используем** (значит LLM‑рендер должен быть максимально детерминированным от NPZ).

**Полуфинальное правило (frontend данные ≠ LLM)**
- Всё, что фронт должен рисовать как “цифры/карточки/графики”, генерируется **детерминированно** из NPZ (не LLM).
- LLM только “озвучивает” и даёт рекомендации, но не является источником структуры/чисел.

**Минимальный contract для LLM‑рендера**
- Вход: компактный “render context” (JSON) из NPZ:
  - ключевые агрегаты, top-issues, top-strengths, highlights по шотам (если есть)
  - флаги missing/empty
- Выход: один текстовый ответ (markdown) + обязательные секции:
  - “что хорошо”, “что плохо”, “3 главных действия”
  - “почему я так решил” (со ссылкой на метрики из context)

Комментарий (ChatGPT):

Рекомендация (полуфинал):
- По умолчанию язык LLM‑текста = `locale` пользователя.
- Если `locale` не задан → RU как дефолт (т.к. сейчас разработка/контент ориентированы на RU).
- В render‑контексте всегда передаём `locale` и `tone` (например: “строго/дружелюбно/коротко”).
---

## 5) Сегментация и семплинг (Segmenter) — ключевая часть

- **цель Segmenter**: что он должен оптимизировать?
  - покрытие всего видео равномерно
  - больше внимания к “событиям” (cuts/motion/лица/текст)
  - минимум затрат при максимальной информативности
- **policy per component**: как Segmenter решает frame_indices отдельно для:
  - `core_clip`
  - `core_depth_midas`
  - `core_face_landmarks`
  - `shot_quality`
  - `cut_detection`
  - `story_structure`, `video_pacing`, etc
- **ограничения на N**: какой допустимый диапазон sampled кадров для каждого компонента?
  - min/max и “идеальный” target
- **стратегия**:
  - равномерная
  - stratified per-shot/per-scene
  - event-driven (flow peaks, OCR changes, face presence)
- **как учитывать длину видео** (120…36000 кадров):
  - какая формула/логика scaling?
- **согласование индексов**:
  - когда индексы должны совпадать между компонентами (пример: `shot_quality` и его dependencies)?
  - когда можно иметь разные индексы и как маппить между ними?
- **временная шкала**: Segmenter оперирует кадрами или временем (seconds/ms)?
- **fps/resolution**: Segmenter нормализует видео (downscale/fps) перед выборкой?

Ответы:

**цель Segmenter** - Я делал Segmenter только для того что бы как то вынести логику выборки кадров в отдельный молуль, но в итоге Segmenter теперь может глобально отвечать за сэмплирование как для VisualProcessor так и для AudioProcessor. Единственная проблема это то что некотырым модулям нужна умная выборка, например если мы хотим проанализировать лица на эмоции мы сначала должны по какому то принципу найти лици а потом по  другому принципу их анализировать, так вот что бы Segmenter мог делать умную выборку для каких то модулей ему самому нужно использовать модели, и тогда получаеться что модели будут  использоваться несколько раз, либо нужно как то переиспользовать данные, вообщем тут нужно хорошо подумать вместе.

**policy per component** - На данный момент для каждого модуля береться равномерная выборка с шагом 5 и это абсолютно не правильно, так как для каждого моджуля должна быть своя логика выбрки кадров, которая изначально должна залодиться в Segmenter

**ограничения на N**
**стратегия**
**как учитывать длину видео**
**согласование индексов**
**временная шкала**
**fps/resolution** - С этими вопросами нужна твоя помощь. Лучшие практики и рекомендации что бы добиться хорошего качества, стабильности.

Рекомендации / комментарии (ChatGPT):

Ключевой риск, который ты правильно заметил: “умная выборка” иногда требует моделей. Чтобы не дублировать тяжёлые модели, предлагаю уровни:

**Segmenter v1 (без тяжёлых моделей)**
- Использует дешёвые сигналы на downscale: histogram diff, brightness, cheap motion proxy.
- Выдаёт `frame_indices` отдельно для каждого компонента.

**Segmenter v2 (двухпроходный, если нужно)**
- Pass1: дешёвые providers (например cut candidates / cheap face presence).
- Pass2: уточнение индексов для дорогих компонентов/модулей.

Полуфинальные правила:
- Segmenter мыслит в time-domain (секунды), но финально отдаёт `frame_indices` (int).
- Каждый компонент имеет budget по N (min/target/max).

Стартовые budgets N (черновик, можно менять):
- `cut_detection`: 400–1500
- `core_clip`: 200–800
- `core_depth_midas`: 120–400
- `core_face_landmarks`: 200–800
- `shot_quality`: 200–1000

**fps/resolution + storage (ключевое для перфа и стабильности)**

Сейчас важно зафиксировать: хранить “все кадры в сыром виде” для видео до 20 минут (до 72k кадров) — это практически гарантированно:
- слишком дорого по диску/IO (десятки гигабайт на видео при raw `uint8`)
- медленно по времени и плохо масштабируется

Поэтому предлагаю полуфинальный контракт:

Полуфинальное правило (analysis timeline):
- Segmenter оперирует временем (секунды) и выбирает кадры **в своей analysis timeline**.
- В `frames_dir/metadata.json` фиксируем:
  - `analysis_fps` (например 12/15)
  - `analysis_height/analysis_width` (downscale)
  - `color_space="RGB"` (важно: все кадры в `frames_dir` храним **RGB**)

Полуфинальное правило (кадры на диске = только union sampled):
- Segmenter выбирает `frame_indices` для каждого компонента.
- Затем строит **union** всех нужных кадров по всем компонентам (`union_frame_indices`).
- В `frames_dir` сохраняем **только union кадры** (в порядке `union_frame_indices`), а не все кадры видео.

Чтобы не потерять связь с исходным видео:
- В metadata сохраняем:
  - `union_frame_indices_source` (индексы/таймстемпы в source timeline, если нужно)
  - и/или `union_timestamps_sec`
- А per-component `frame_indices` в metadata — это **индексы в union-буфере** (0..N-1), которые валидны для `FrameManager.get()`.

Это даёт:
- гарантированный upper bound по хранению (сотни/тысячи кадров, а не десятки тысяч)
- единый RGB контракт для всех модулей
- отсутствие “магии” и воспроизводимость

Новый вопрос (ответь ниже):
1) Ок ли тебе принять контракт: `frames_dir` хранит **только union sampled кадры**, а `frame_indices` в metadata — это индексы в этом union (не “сырой” индекс исходного видео)?

Ответы:

1. Идея норм

Комментарий к ответу (ChatGPT, раунд 2):

Отлично — фиксируем как полуфинальный стандарт:
- `frames_dir` содержит **только union sampled** кадры (в analysis timeline).
- Любые `frame_indices` в metadata для компонентов — это **индексы в union** (0..N-1), валидные для `FrameManager.get()`.
- Для связи с исходником храним mapping: `union_timestamps_sec` и/или `union_frame_indices_source`.

Важно (implementation note):
- Нам нужно привести `Segmenter`/`FrameManager` к одному формату батчей + добавить явное поле `color_space` (RGB), чтобы убрать BGR/RGB дрейф.

Новые вопросы (ответь ниже):
1) Segmenter может быть двухпроходным (Pass1→Pass2), или строго один проход?
2) Segmenter должен выдавать shots/segments как артефакт, или это задача только `cut_detection`?

Ответы:

1. Segmenter может быть двухпроходным (как ты скажешь), главное добиться стабильности и качества при большом разбросе кадров (длительности видео)
2. shots/segments идут уже как фичи и для этого есть отдельный модуль `cut_detection`. Segmenter этим не занимаеться

Комментарий к ответам (ChatGPT, раунд 2):

**Фиксируем**
- Segmenter допускает двухпроходный режим.
- Shots/segments = ответственность `cut_detection`, Segmenter не формирует сегменты.

**Как сделать двухпроходность без дублирования тяжёлых моделей**
- Полуфинальное правило: Pass1 Segmenter использует только дешёвые сигналы (downscale) и/или лёгкие core результаты.
- Если `cut_detection` тяжёлый, то Segmenter Pass1 не должен запускать его “внутри себя” — лучше иметь отдельный лёгкий “cut candidates” сигнал, либо запускать `cut_detection` как отдельный компонент до модулей, если он нужен многим.

Новый вопрос (ответь ниже):
1) Ок ли вынести “cut candidates (cheap)” как часть Segmenter (без ML), а `cut_detection` оставить как отдельный модуль, который считает финальные cuts?

Ответы:

1. Да идея хорошая
---

## 6) Контракты между процессорами (Audio/Visual/Text)

### 6.1 Общий контракт артефактов
- **единый storage**: где хранятся артефакты? один `result_store` на всё или отдельно per-processor?
- **структура директорий**: `rs_path/<component>/...` — это финальный стандарт для всех?
- **формат**: ты подтвердил 100% NPZ — значит:
  - нужны ли JSON вообще (например, для аналитиков)?
- **human-friendly слой**: ты хочешь третий “analytics output”:
  - это отдельные parquet/csv?
  - или отдельный “render” в json (генерируемый из npz)?

Ответы:

**единый storage** - на данный момент этот вопрос только начал обрабатываться и есть нароботки result_store в VisualProcessor, но по факту четких правил нет. Нужно четко разобраться с этим вопросом. Нужны советы и рекомендации.

**структура директорий** - это не финальный стандарт, нужно также решить этот впорос

**формат** - JSON нужен скорее тогда когда мы передаем данные на backend/frontend для пользователя как финальные результаты

**human-friendly слой** - пока мы не думаем об этом так как для первой версии проекта мы не будем углубляться в создание алгоритмов для проф. аналитиков.

Рекомендации / комментарии (ChatGPT):

Полуфинальные правила:
- Источник истины = NPZ.
- JSON = presentation layer (генерируемый из NPZ) для backend/frontend.

Новый вопрос (ответь ниже):
1) Ок ли сделать storage per-run: `result_store/<platform>/<video_id>/<run_id>/...` (чтобы параллельные пользователи/прогоны не конфликтовали)?

Ответы:

1. Думаю что ок, если ты считаешь что так правильно то да

Комментарий к ответам (ChatGPT, раунд 2):

**Фиксируем**
- Storage per-run принимаем как стандарт.

Полуфинальное правило (структура путей):
- `result_store/<platform_id>/<video_id>/<run_id>/<component_name>/...`
- Дополнительно: `result_store/<platform_id>/<video_id>/<run_id>/manifest.json` (список артефактов, версии, статусы)

Новый вопрос (ответь ниже):
1) Ты хочешь хранить `manifest.json` рядом с артефактами (удобно для backend), или всё будет только через БД?

Ответы:

1. Реши сам, как будет правильнее, эффективнее

Комментарий к ответам (ChatGPT, раунд 3):

Выбираю гибрид (и фиксируем как полуфинальный стандарт):

**`manifest.json` рядом с артефактами — да**
- Это самый простой “source of truth” для каждого `run_id`, не требует БД, легко дебажится и удобно для backend.

**БД (artifact index) — тоже да, но как ускоритель**
- БД хранит индексы/статусы/агрегаты и позволяет быстро находить подходящий run по ключу (кэш, дедуп, поиск).
- Но даже если БД временно недоступна, `manifest.json` позволяет восстановить состояние.

Полуфинальное правило:
- `manifest.json` обязателен для каждого run.
- БД опциональна на MVP, но схема БД проектируется сразу под “artifact index”.

### 6.2 Зависимости TextProcessor ↔ VisualProcessor
- **как Visual OCR текст попадает в TextProcessor**:
  - как именно хранится OCR output? (per-frame tokens, per-segment text, timestamps)
  - как решаем дубликаты текста (одна и та же надпись много кадров)?
- **комментарии по снапшотам**:
  - TextProcessor обрабатывает 4 снапшота отдельно или объединяет?
  - нужны ли временные фичи по комментам (динамика токсичности/тем)?
- **конфликт языков**: OCR RU, комменты EN — какой pipeline?
- **идентификатор видео**: как связываем артефакты (video_id/путь)?

Ответы:

**как Visual OCR текст попадает в TextProcessor** - Идея такая: VisualProcessor выполняет свою часть, ищет текст на кадрах. TextProcessor - выполняет свою часть, анализирует текст, возможно сравнивает с другими тестами от этого видео и тд. На счет передачи нужно сербезно подумать, как это оптимпльно реализровать.

**комментарии по снапшотам** - TextProcessor обрабатывает 4 снапшота раздельно, но логично что на 2 - 3 снапшотах могут быть новые комментарии при это старые тоже остануться, то есть нужно кэширование старых  и другие оптимизации, также я думаю стоит высчитывать какую то динамику по коментам относительно снапшотов.  

**конфликт языков** - Нужны твои советы

**идентификатор видео** - video_id

Рекомендации / комментарии (ChatGPT):

**OCR → TextProcessor**
- Контракт (предложение): VisualProcessor сохраняет NPZ `visual_ocr`:
  - `frame_indices`, `text_present`, `texts`, `boxes`, `conf`, `lang`
  - плюс “dedup segments” (time ranges), чтобы не тащить повторы текста.
- TextProcessor читает `visual_ocr` как dependency.

**Комментарии и динамика**
- Практика: хранить union комментариев по id (если доступен) и считать delta-фичи (new_comments_count, topic shift, etc.).

**Мультиязычность**
- MVP: детект языка отдельно по источникам и использовать multilingual embeddings (без перевода).

Новые вопросы (ответь ниже):
1) TextProcessor должен работать, если OCR выключен/не доступен?
2) OCR текст в проде: храним raw полностью или только агрегаты/эмбеддинги (privacy)?

Ответы:

1. TextProcessor должен работать
2. На самом деле можно сделать это опциональным, например если в хранении raw есть плюсы для пользователя, он может включить эту опцию на сайте и в его личном кабинете под нужным анализом будут доступны текста, но при этом нужно соблюдать правовые нормы в которых я не сильно разбираюсь но думаю что пользовательно должен дать какие то подтверждения.

Комментарий к ответам (ChatGPT, раунд 2):

**TextProcessor без OCR — фиксируем**
- Полуфинальное правило: TextProcessor не должен падать без OCR; просто часть фичей будет missing (`*_present` masks).

**Raw OCR как опция**
- Это разумно. Чтобы не сломать privacy/ToS, предлагаю контракт:
  - по умолчанию: сохраняем только derived (эмбеддинги/агрегаты) и удаляем raw OCR текст
  - опция пользователя: “retain raw text” (с явным согласием), и тогда raw хранится ограниченное время

Новые вопросы (ответь ниже):
1) Хочешь ли ты “retention TTL” для raw текста (например 7/30 дней), даже если пользователь включил опцию?
2) Если raw включён: мы храним только OCR текст, или ещё и raw comments? (это разные уровни риска)

Ответы:

1. Я бы обошелся без “retention TTL” или вынести это также как параметр который указывает пользователь
2. Для коментариев отдельная опция. Я говорю про опции и подтверждения в том смысле что как я понимаю хранить эти данные может только автор канала если он дал подтверждения и доказательства на моем сайте

Комментарий к ответам (ChatGPT, раунд 3):

**TTL: предлагаю всё-таки иметь hard cap (даже если пользователь выбирает)**
- Без TTL можно очень быстро “утонуть” в storage + появляется юридический риск (хранение PII без ограничений).
- Компромисс:
  - пользователь выбирает TTL (или “без TTL”),
  - но система применяет `max_retention_days` (hard cap), например 30/90 дней.

Полуфинальное правило:
- `retain_raw_ocr_text` = false по умолчанию.
- `retain_raw_comments` = false по умолчанию (отдельная опция, как ты сказал).
- Если raw включён → обязателен `retention_days` + применяется `hard_cap_days`.

**“Только автор канала может хранить” — нужно формализовать верификацию**
- Предлагаю единственный безопасный путь: OAuth/verification через платформу (YouTube):
  - пользователь логинится,
  - мы получаем подтверждение “каким каналом он владеет” (channel_id),
  - разрешаем raw storage только для его channel_id.
- “Доказательства на сайте” без API-верификации трудно автоматизировать и легко обойти.

Новый вопрос (ответь ниже):
1) Готов ли ты принять правило: raw storage разрешён только после OAuth-верификации владельца канала (YouTube)?

Ответы:

1. Да
---

## 7) VisualProcessor: как ты хочешь видеть “финальный” продукт

- **граница ответственности**:
  - VisualProcessor считает только фичи или также управляет моделями (Triton)?
- **компоненты core vs module**:
  - какие модели должны быть строго в core/model_process?
  - какие допустимы внутри модулей?
- **перенос на Triton**:
  - какие модели кандидаты №1 (CLIP, YOLO, MiDaS, face landmarks)?
  - формат батчинга/параллелизм
  - ожидания по latency/throughput
- **какие модули критичны для MVP**:
  - минимальный набор для обучения первых моделей
- **что является “sequence output”** для VisualTransformer:
  - per-frame embedding
  - per-shot embedding
  - per-scene embedding
  - energy curves / pacing curves
- **какие агрегаты важны для Fusion**:
  - общий quality score, pacing stats, uniqueness proxy и т.п.
- **интерпретируемость**: какие именно артефакты должны быть удобны аналитикам?

Ответы:

**граница ответственности** - также управляет моделями Triton

**компоненты core vs module** - внутри core только те модели которые используються больше чем в одном модуле. В модулях используються модели которые использують только в них и нигде больше.

**какие модули критичны для MVP** - максимально кол-во для достижение лучшего качетсва предсказания

**что является “sequence output”** - Тут нужен твой совет, но я думаю что должны принимать участия все виды (per-frame, per-shot, per-scene и тд.)

**какие агрегаты важны для Fusion** - нужны твои советы и лучшие практики

**интерпретируемость** - давай пока сделаем что то среднее, то есть что бы интерпретируемость была но не будет сильно углубляться. Это не первостепенная задача.

Рекомендации / комментарии (ChatGPT):

**Sequence outputs (уточнение после нашего решения token=shot)**
- Для training schema считаем “основной” sequence output = **per-shot**.
- Per-frame/per-scene/curves можно хранить как optional/debug/analytics, но не делать обязательными для MVP‑обучения.

**Какие агрегаты важны для Fusion (best practices, Tier‑0)**
Цель агрегатов: дать сильный табличный сигнал (baseline) и стабильные summary‑фичи для v2.

Предлагаю фиксировать минимальный набор агрегатов (примерно 30–80 скаляров), сгруппированных так:
- **Visual quality / aesthetic**
  - mean/median/p10/p90 “quality” по шотам
  - доля “плохих” шотов (ниже порога)
- **Pacing / editing**
  - `n_shots`, `shots_per_min`
  - stats по длительности шотов (mean/median/p10/p90)
  - доля “быстрых” cuts (<X sec), доля “длинных” (>Y sec)
- **Motion / dynamics (cheap + стабильные)**
  - proxy motion energy stats (если есть в `video_pacing`/Segmenter)
  - доля “статичных” интервалов
- **Content diversity / uniqueness**
  - разнообразие сцен/объектов (entropy / unique count / top-K share)
  - “repetition ratio” (доля повторяющихся шотов/кадров по эмбеддингам)
- **Faces / people (если есть)**
  - доля времени с лицами (mask)
  - базовые stats по pose/eye-contact proxy (если включено)
- **Text/OCR presence (если включено)**
  - доля кадров/шотов с текстом
  - динамика смены текста (сколько уникальных “сегментов”)

Полуфинальное правило:
- Любой агрегат, который может отсутствовать, обязан иметь `*_present` и/или быть NaN (без нулевых “заглушек”).

Новый вопрос (ответь ниже):
1) Ок ли тебе, что для MVP мы фиксируем “tabular aggregate set” как **обязательный выход** VisualProcessor (для baseline и для v2), а более детальные ряды — optional?

Ответы:

1. Да, но более детальные ряды не optional, просто будут в версии v2

Комментарий к ответу (ChatGPT, раунд 2):

Ок, фиксируем более точную формулировку:

Полуфинальное правило (outputs по уровням):
- **Обязательный минимум для MVP (Tier‑0, training schema)**:
  - tabular aggregates (скаляры/векторы небольшой размерности)
  - per-shot sequence (до `max_len_shots=256`) — для v2
- **v2 (prod) включает “детальные ряды” как стандарт**:
  - sequences/curves/highlights, нужные для multimodal transformer и адекватного объяснения результата
- **baseline/v1** могут обучаться/работать только на агрегатах (и это нормально) — как sanity-check и fallback.

Новый вопрос (ответь ниже):
1) Под “детальные ряды v2” ты хочешь в первую очередь:
   - (a) per-shot embeddings (`E_shot`, `A_shot`)
   - (b) pacing curves (motion/energy over time)
   - (c) story structure / segments
   - (d) другое (уточни)

Ответы:

1. Все хочу (если нужно расставить приоритеты сделай сам)

Комментарий к ответу (ChatGPT, раунд 3 — приоритизация v2):

Ок, тогда приоритеты для v2 предлагаю такие (чтобы быстрее получить качество и не утонуть в разработке):

- **P0 (самое важное для v2 качества)**:
  - (a) per-shot embeddings `E_shot` (visual) и `A_shot` (audio) + masks (это “кровь” мультимодального трансформера)
- **P1 (сильно помогает модели и объяснимости)**:
  - (b) pacing curves / motion-energy over time (можно в shot-domain и/или time-domain)
- **P2 (следующий слой, но можно чуть позже)**:
  - (c) story structure / segments (как дополнительная структура поверх cuts)

Полуфинальное правило:
- Все детальные ряды должны иметь ясные budgets/shape/masks и быть воспроизводимыми (с `schema_version`).

**Triton**
- Рекомендую Triton как отдельный сервис (serving), а VisualProcessor как клиент (endpoint + model versions).
- “Управление Triton” внутри VisualProcessor обычно усложняет продакшн: лучше разделить ответственность.

**MVP “максимум модулей”**
- Риск: много модулей может добавить шум и ухудшить обучение.
- Полуфинальное правило: каждый новый модуль включаем флагом и проверяем вклад на валидации (A/B).

Новый вопрос (ответь ниже):
1) Ты готов принять правило “сначала baseline + ключевые модули, потом расширение”, или хочешь реализовать максимум модулей до первого обучения?

Ответы:

1. принимаю правило “сначала baseline“
2. С Triton согласен, теперь нужно определить как это все будет взаимодействовать

Комментарий к ответам (ChatGPT, раунд 2):

**Как будет взаимодействовать Triton и VisualProcessor (полуфинальный контракт)**
- Triton = отдельный сервис model-serving (версии моделей, batching, GPU scheduling)
- VisualProcessor = клиент:
  - формирует батчи
  - отправляет запросы в Triton по gRPC/HTTP
  - получает выходы и пишет NPZ

Полуфинальные правила:
- В `meta` артефактов фиксируем `model_name` + `model_version` (из Triton), чтобы результаты были воспроизводимы.
- VisualProcessor не “поднимает” Triton, а “подключается” к нему (endpoint + timeouts + retries).

Новый вопрос (ответь ниже):
1) В dev-режиме Triton будет запускаться локально (docker-compose), или dev тоже будет ходить в удалённый Triton?

Ответы:

1. в dev-режиме Triton будет запускаться локально 
---

## 8) NPZ-схемы: как договоримся о стабильности

- **обязательная meta-секция**:
  - какие поля должны быть в `meta` в каждом NPZ (producer, version, created_at, input video id, config hash)?
- **nullable данные**:
  - ты хочешь видеть `None` — в NPZ это обычно:
    - `*_present` mask + `NaN` в массиве + `empty_reason` в meta
  - ок ли такой стандарт везде?
- **строгая валидация схемы**:
  - хотим ли писать маленький валидатор, который проверяет ключи/формы каждого NPZ?

Ответы:

**обязательная meta-секция** - посоветуй как лучше сделать

**nullable данные** - сделаем как ты считаешь правильным

**строгая валидация схемы** - думаю что стоит это сделать

Рекомендации / комментарии (ChatGPT):

Полуфинальное правило: обязательная `meta` во всех NPZ:
- `producer`, `producer_version`, `schema_version`
- `created_at`
- `platform_id`, `video_id`, `run_id`
- `config_hash`, `sampling_policy_version`
- `status` = ok/empty/error, `empty_reason` (если empty)

Валидация:
- сделать простой валидатор схем (python), который проверяет:
  - ключи, dtype, shape
  - `frame_indices` отсортированы и уникальны
- запускать валидатор либо в runtime (после каждого компонента), либо в CI (на тестовых артефактах).

Новые вопросы (ответь ниже):
1) Схемы фиксируем как `SCHEMA.md` рядом с модулем, или как machine-readable `schemas/*.json`?
2) Валидатор важнее в CI или прямо в pipeline runtime?

Ответы:

1. Реши сам как будет лучше
2. Реши сам как будет лучше

Комментарий к ответам (ChatGPT, раунд 2):

Ок, выбираю так (и фиксируем как полуфинал):

**Как хранить схемы**
- Делать оба слоя:
  - `SCHEMA.md` рядом с модулем (human-friendly, для Cursor/нейросети)
  - machine-readable schema (например `schemas/<component>.json`) для валидатора/CI

**Где запускать валидатор**
- И там, и там:
  - runtime: сразу ловить битые/неполные артефакты (особенно важно в проде)
  - CI: чтобы изменения схемы не ломали совместимость

Новый вопрос (ответь ниже):
1) Ок ли тебе структура: `VisualProcessor/schemas/*.json` как единый реестр машинных схем?

Ответы:

1. Ок
---

## 9) Оркестрация пайплайна и выполнение

- **сейчас**: запуск subprocess’ами из `VisualProcessor/main.py`.
  - это останется или будет единый orchestrator в DataProcessor?
- **DAG зависимостей**:
  - хотим ли явно описывать граф (например, `shot_quality` зависит от core_* и cut_detection)?
  - кто строит DAG: конфиг, код, или отдельная система?
- **celery**:
  - какие единицы задач? (core providers, modules, per-video, per-segment)
  - гарантии: retries, idempotency
- **кэширование**:
  - если пересчёт только одной части — как не пересчитывать всё?
- **наблюдаемость**:
  - какие логи/метрики нужны (per-module time, gpu mem, failures)?

Ответы:

**сейчас** - посоветуй как лучше сделать

**DAG зависимостей** - посоветуй как лучше сделать

**celery** - посоветуй как лучше сделать

**кэширование**  - посоветуй как лучше сделать

**наблюдаемость** - метрики системы точно нужны, остальное как ты скажешь

Рекомендации / комментарии (ChatGPT):

Полуфинальная стратегия:
- Оркестратор должен быть на уровне DataProcessor (единый DAG).
- Каждая задача должна быть idempotent по ключу:
  - `(platform_id, video_id, run_id, component, config_hash, sampling_policy_version)`.
- Кэширование по тем же ключам: если артефакт уже существует — не пересчитываем.

Новые вопросы (ответь ниже):
1) Режим pipeline: **fail-fast** или **best-effort**?
2) Celery в будущем: задача “1 видео” или “1 компонент”?

Ответы:

1. Не совсем понимаю что это значит, но скажу что нам нужно качество
2. реши сам как будет лучше, но объясни мне

Комментарий к ответам (ChatGPT, раунд 2):

**fail-fast vs best-effort (объяснение)**
- **fail-fast**: если упал обязательный компонент (например `core_clip`) — весь пайплайн останавливается. Это даёт “качество/предсказуемость”, но меньше результатов, если что-то ломается.
- **best-effort**: если упал один компонент — остальные продолжают. Это даёт больше “частичных результатов”, но усложняет downstream (много missing).

Полуфинальное решение (гибрид, лучшее для качества):
- Для **обязательных** компонентов (для модели/основных фичей) — fail-fast.
- Для **опциональных** (аналитика/доп. фичи) — best-effort.
- Значит нам нужен список “required components” для training schema.

**Celery: 1 видео или 1 компонент**
- На старте проще и надёжнее: “1 видео = 1 job”, внутри него orchestrator запускает компоненты по DAG.
- Потом, когда будет масштабирование: “1 компонент = 1 task” (лучше параллелизм, но сложнее кэш/идемпотентность).

Новый вопрос (ответь ниже):
1) Ты согласен на гибридный режим: required = fail-fast, optional = best-effort?

Ответы:

1. Да

---

## 10) Базы данных и хранение

- **что храним в БД**:
  - ссылки на артефакты
  - агрегаты (для быстрых запросов)
  - сырые последовательности (обычно нет)
- **формат аналитики**:
  - parquet таблицы по видео/шотам/кадрам?
- **retain policy**:
  - ты сказал “позже” — но какие факторы будут влиять? (стоимость, аудит, дебаг)

Ответы:

**что храним в БД** - оттолкнемся от твоих рекомендаций

**формат аналитики** - как ты посоветуешь

**retain policy** - давай пока будет хранить небольшую часть последних прогонов. Сделаем по твоим советам

Рекомендации / комментарии (ChatGPT):

Полуфинальные правила:
- В БД хранить:
  - ссылки на артефакты
  - агрегаты (video-level / shot-level)
  - статусы прогонов
- Сырые frame-level последовательности в БД не класть.

Новый вопрос (ответь ниже):
1) Какую БД планируешь (PostgreSQL / другое), или пока не решил?

Ответы:

1. Пока не решил, но как минимум у меня есть опыт с PostgreSQL

Комментарий к ответам (ChatGPT, раунд 2):

Рекомендация для MVP:
- PostgreSQL как primary DB (runs, statuses, pointers to artifacts, aggregates).
- Артефакты (NPZ) — в object storage (S3/MinIO) или файловое хранилище на старте.

Новый вопрос (ответь ниже):
1) Ты хочешь на старте хранить NPZ просто на диске (локально), или сразу закладываться на S3/MinIO?

Ответы:

1. Я не хочу пока платить за S3 (типо Selectel), если есть бесплатные варианты, то я готов сразу закладываться на S3/MinIO

Комментарий к ответам (ChatGPT, раунд 3):

Ок — тогда полуфинальная рекомендация:

**Storage на MVP**
- Dev: локальный диск.
- Prod/MVP: self-hosted **MinIO** (S3-compatible). Это бесплатно (кроме твоих серверов) и потом легко мигрируется на платный S3.

Полуфинальное правило:
- В коде делаем “storage adapter” (filesystem vs s3), чтобы смена backend не ломала pipeline и пути артефактов.

---

## 11) Обучение моделей и эксперименты

- **train/val/test split**:
  - по времени? по каналам? по категориям?
- **data leakage**:
  - если используем снапшоты после публикации, важно не “подсмотреть” будущее.
  - что считается допустимым входом на момент предсказания?
- **experiment tracking**:
  - нужен ли MLflow/W&B?
- **репродуцируемость**:
  - как фиксировать версии фичей + версию Segmenter sampling policy?

Ответы:

**train/val/test split** - скажи ты как лучше

**data leakage** - (надеюсь я правильно понял вопрос) на момент предсказания в модель подаються данные только первого снапшота, остальны 3 снапшота являюьться как бы таргетами, которые модель должна предсказать (то есть просмотры через неделю, две и три недели, то есть если грубо в модель подается первый снапшот и она выдает 3 значения - кол-во просмотров через 1, 2 и 3 недели - это как baseline. Если ты расскажешь мне подробнее и посоветуешь как вокруг этого можно еще сделать предсказание например лайков, подписчиков и тд. без кучи моделей)

**experiment tracking** - однозначно нужен

**репродуцируемость** - посоветуй как лучше

Рекомендации / комментарии (ChatGPT):

**Split**
- Рекомендую split по времени публикации + группировка по каналу (channel-group split), чтобы один канал не попал в train и test одновременно.

**Leakage**
- Твой подход звучит корректно: inputs = snapshot1, targets = future snapshots.
- Полуфинальное правило: “вход модели = только snapshot1 (никаких future-derived данных)”.

**Reproducibility**
- фиксировать:
  - `dataprocessor_version`
  - `schema_version` всех артефактов
  - `sampling_policy_version`
  - commit hash
  - seed

Новые вопросы (ответь ниже):
1) `publishedAt` есть у всех видео и доверяем ли этой дате для time-split?
2) Хочешь multi-horizon (7/14/21/30) или строго 30 дней?

Ответы:

1. Да, доверяем `publishedAt`
2. Хочу, для этого я и собирал снапшоты через неделю, но нужна помощь с multi-horizon

Комментарий к ответам (ChatGPT, раунд 2):

**Multi-horizon (как сделать правильно)**
- Так как counts в снапшотах кумулятивные, лучше предсказывать **дельты** (приросты) по горизонтам.
- Практика:
  - `y14 = log1p(delta_views_14d)`
  - `y21 = log1p(delta_views_21d)`
  - аналогично likes/comments/subs
- Это хорошо сочетается с multi-target: одна модель → несколько горизонтов → несколько метрик.

Новый вопрос (ответь ниже):
1) Подтверди, пожалуйста, что интервал между снапшотами всегда 7 дней и горизонты фактически 7/14/21 (а не 30).

Ответы:

1. интервал между снапшотами всегда 7 дней (+- несколько часов). Выше я уже говорил что интервал 7 закосячен и по факту нормальные только 14/21
---

## 12) Разработка “одним человеком + Cursor”

- **как ты хочешь, чтобы нейросеть рефакторила**:
  - предпочтения по стилю (больше классов/больше функций/минимум абстракций)
  - что обязательно документировать в каждом модуле (README + FEATURES_DESCRIPTION + schema)
- **правила изменения контрактов**:
  - если меняем ключи/формы в NPZ — нужно ли автоматически обновлять документацию и писать миграцию?

Ответы:

**как ты хочешь, чтобы нейросеть рефакторила** - Очень важный вопрос, так как если правильно сформулировать процесс разработки, то одна нейросеть сможет закончить проект очень качественно почти без большой затраты человеческих усилий. Основная проблема это контекст нейросети и умения понимать глобальную логику. То есть во-первых мы должны делать упор на модульную архетектуру что бы нейросеть могла сконцетрировать внимание на конкретном модуле и пытаться анализировать всю сложную архетектуру и логику проекта, во-вторых мы должны писать документации так и в таком кол-ве что бы нейросеть могла прочитать минимум информации и понять осноауную суть всего проекта и суть конкретной задачи и качественно ее решить. Пример: в VisualProcessor есть модуль emotion_face в котором четко описан вход, например 10 кадров от какогото видео, далее я даю задачу неросети реализовать какие то алгоритмы для новых фичей, так вот нейросеть должна прочитать один-два файла документации и понять как качественно реализровать новые фичи опираясь на глобальную задачу проекта и суть конретного модуля, понимать какой должен быть выход и куда он пойдет.

**правила изменения контрактов** - обязательно

Рекомендации / комментарии (ChatGPT):

Полуфинальное правило:
- Любое изменение NPZ schema:
  - bump `schema_version`
  - обновить README/SCHEMA
  - запись в CHANGELOG
  - по возможности поддержать legacy loader (или миграцию)

Новый вопрос (ответь ниже):
1) Ты хочешь автогенерацию SCHEMA из кода, или ручное ведение (как сейчас)?

Ответы:

1. Я думаю что то между, то есть автогенерация с моей ручной корекцией

Комментарий к ответам (ChatGPT, раунд 2):

Отлично. Полуфинальный процесс:
- генерация machine-schema (`schemas/*.json`) из кода (автоматически)
- `SCHEMA.md` можно генерировать из json, но допускается ручная “полировка” (описания, примеры)
- CI проверяет, что json schema обновлена, если поменялись ключи/формы

Новый вопрос (ответь ниже):
1) Ок ли правило: “если поменяли schema в коде — PR обязан включать обновление `schemas/*.json`”?

Ответы:

1. Да

---

## 13) Минимальный MVP и план работ

- **какой MVP нужен первым**:
  - только Visual+Audio? без Text?
  - только агрегаты? или уже sequence для трансформеров?
- **порядок приведения модулей**:
  - какие модули сейчас самые важные для первых моделей (по твоему мнению)?
- **критерии готовности модуля**:
  - “готов” = стабильный NPZ schema + docs + валидатор + perf budget?

Ответы:

**какой MVP нужен первым** и **порядок приведения модулей** - мы должны добиться хорошего качества предсказания. Для этого мы долны реализолвать максимальный функционал для моделей что бы после запуска проекта в продакшн мы уже не переобучали модели, а изменяли только те алгоритмы которые выдают фичи например для аналитиков

**критерии готовности модуля** - стабильный NPZ schema + docs + валидатор + perf budget

Рекомендации / комментарии (ChatGPT):

Полуфинальная рекомендация:
- Сделать “priority list v1” из 5–7 компонентов, нужных для первого обучения, чтобы не распыляться.

Новый вопрос (ответь ниже):
1) Ты готов утвердить priority list v1 (5–7 модулей/провайдеров)?

Ответы:

1. Да, но я бы взял чуть больше модулей, например несколько из каждого процессора

Комментарий к ответам (ChatGPT, раунд 2):

Согласен: “5–7” — это чтобы не распыляться, но можно сделать компромисс:

Полуфинальное правило (tier-список):
- Tier-0 (required для training schema): небольшой набор, который точно стабилен и даёт сильный сигнал.
- Tier-1 (recommended): расширение, которое можно включать после baseline.

Новый вопрос (ответь ниже):
1) Выбери 2–3 самые важные модальности для качества на первом цикле: Visual / Audio / Text / Temporal?
2) Ок ли тебе tier-0 = 8–10 компонентов (вместо 5–7), а остальное tier-1?

Ответы:

1. Visual, Audio, Temporal
2. tier-0 = 10 компонентов, остальное tier-1

Комментарий к ответам (ChatGPT, раунд 3):

Ок, фиксируем: Tier-0 фокус = Visual + Audio + Temporal.

Чтобы дальше не было “размытости”, предлагаю зафиксировать Tier-0 как **список из 10 компонентов** (это могут быть как processors, так и modules/core providers).

Черновик Tier-0 (предложение, поправим под реальные названия в коде):
- **Temporal (Fetcher/TextProcessor-side)**:
  1) `temporal_snapshot_features` (из snapshot1: counts/rates/derivatives + masks)
  2) `targets_builder_multi_horizon` (формирует y14/y21 и masks `has_horizon_*`)
- **Visual (VisualProcessor)**:
  3) `core_clip`
  4) `cut_detection`
  5) `shot_quality`
  6) `scene_classification`
  7) `video_pacing` (агрегаты pacing/energy curve)
- **Audio (AudioProcessor)**:
  8) `audio_embedding_core` (название уточним)
  9) `audio_quality/pacing` (название уточним)
  10) `speech_presence_or_asr` (название уточним)

Новые вопросы (ответь ниже):
1) Какие 3 “обязательных” компонента в AudioProcessor ты хочешь в Tier-0 (по названиям из репозитория)?
2) Visual Tier-0 ок ли в составе: `core_clip`, `cut_detection`, `shot_quality`, `scene_classification`, `video_pacing`?

1. вот папка: AudioProcessor/src/extractors - проанализируй и выбери сам
2. Да

Комментарий к ответам (ChatGPT):

Я предлагаю Tier-0 для AudioProcessor (3 обязательных экстрактора фичей):

1) `clap_extractor` (семантические аудио-эмбеддинги)
- Даёт сильный high-level сигнал (тип/жанр/“впечатление” от звука), хорошо подходит для Fusion/Transformer.
- Требует `laion_clap` + `torch/torchaudio`, но это ок для Tier-0 (у тебя фокус на качество + есть GPU ресурсы).

2) `loudness` (RMS/peak/dBFS + статистики short-term динамики, LUFS опционально)
- Дешёвый, стабильный CPU-экстрактор.
- Хорошо отражает “профессиональность” микса и динамику (коррелирует с perceived quality).

3) `tempo` (BPM + confidence + optional windowed BPM)
- Дешёвый, стабильный CPU-экстрактор.
- Хороший сигнал “пейса” в аудио (ритм/монтаж/энергия), важен для short-form контента.

Почему не беру в Tier-0 речь/ASR:
- `asr_extractor` и особенно `speech_analysis_extractor` (ASR+diarization) — дорогие, сильно завязаны на модели/языки/ошибки и лучше как Tier-1 (optional) для улучшений и аналитики.

Отдельно (обязательный тех. шаг, но не считаю как “фича”):
- `video_audio_extractor` должен выполняться первым, чтобы стабильно получить WAV из видео и метаданные (duration/sr/etc).

---

## 14) Доп. уточняющие вопросы (короткие)

- **стандарт имён**: `core_depth_midas` vs `depth_midas` (в конфиге встречается `depth_midas`) — какое имя считаем каноничным в metadata и rs_path?
- **единый идентификатор видео**: где он хранится (metadata.json?) и должен ли быть во всех `meta` внутри NPZ?
- **порог “слишком много кадров”**: при каких N мы считаем “длинное видео” и меняем sampling policy?

Ответы:

**стандарт имён** - как ты посоветуешь

**единый идентификатор видео** - тут нужно учитывать не только процесс разработки и обучения, но и дальнейшее испольщование алгоритмов в продакшене, так что нужно четко решить как будет храниться и передаваться идентификатор, например когда 10 человек запускают анализ на сайте, то на ml-service приходят запросы на 10 видео (а может быть такое что будут дублткаты, если например 2 человека хотят анализ одного видео) и нужно решить как все процесоры и модули внутри низ будут понимать какие данные к каким видео относяться. Нужны твои советы

**порог “слишком много кадров”** - хороший вопрос, тут нужно понимать что обучения моделей будет происходить на видео не длинее чем 20 минут и если это 60 fps то максимум 72000 кадров, но как я думаю в дальнейшем модель сможет обрабатывать и более длинные видео. Все упираеться во время обработки и мощность серверов. Давай пока думать так что максимальная длинна видео - 20 минут и макс. кол-во кадров - 72000. Отталкиваясь от этого нужно разработать качественную sampling policy. Нужны твои советы.

Рекомендации / комментарии (ChatGPT):

**Стандарт имён**
- Рекомендация: каноничные имена везде (metadata, config, rs_path):
  - `core_clip`, `core_depth_midas`, `core_object_detections`, `core_face_landmarks`

**Идентификаторы**
- Полуфинальное правило: в `meta` каждого NPZ всегда есть `platform_id`, `video_id`, `run_id`.

**Порог кадров и sampling**
- Полуфинальное правило: Segmenter всегда лимитирует budget N per component и работает в time-domain.

Новые вопросы (ответь ниже):
1) Ты готов сделать `run_id` обязательным входом DataProcessor (приходит с backend), а для offline/batch генерировать внутри?
2) `platform_id` фиксируем строкой `"youtube"` на текущем этапе?

Ответы:

1. Да
2. Да

Комментарий к ответам (ChatGPT, раунд 2):

Отлично — это снимает большую часть проблем с idempotency и параллельными запросами.

Полуфинальные правила:
- `platform_id="youtube"` фиксируем на текущем этапе.
- `run_id` обязателен для прод-режима (приходит извне), для offline/batch может генерироваться внутри.
- Каноничные имена компонентов фиксируем (без alias-ов в metadata/rs_path).

Новый вопрос (ответь ниже):
1) Ок ли правило: если в конфиге/коде есть alias (например `depth_midas`), то мы делаем только “alias mapping” на входе, а везде внутри используем каноничное `core_depth_midas`?

Ответы:

1. Да нужео просто убрать из кода alias, и привести все к каноничному, например `core_depth_midas`

Комментарий к ответам (ChatGPT, раунд 3):

Согласен. Полуфинальное правило:
- Внутри кода/metadata/rs_path используем **только каноничные имена**.
- Aliases допускаются только как “compat слой” на этапе чтения старых конфигов/артефактов (и помечаются как deprecated).
- После миграции — удаляем aliases полностью.