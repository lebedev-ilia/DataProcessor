## TrendFlow — MODELS Q&A (Round 1)

Формат работы:
- Я добавляю вопросы в конец файла.
- Ты отвечаешь прямо под каждым вопросом (кратко, но однозначно).
- Если по ответам появляются новые вопросы — я дописываю их ниже отдельным блоком "Round N+1".

---

### Round 1 — вопросы

#### 1) Версионирование моделей и воспроизводимость

- **Q1. Версионирование моделей в NPZ meta**: в документации упоминается, что в `meta` каждого NPZ нужно фиксировать `model_name` и `model_version` (если использовался Triton/ML модель). Как именно это должно работать:
  - для core providers (CLIP, MiDaS, YOLO, RAFT, Mediapipe) — фиксируем версию модели в `producer_version` или отдельное поле `model_version`?
  - для модулей, использующих модели (emotion_face, scene_classification, etc.) — как версионируем?
  - для моделей из HuggingFace/transformers — используем полный путь (например, `"openai/clip-vit-base-patch32"`) или отдельно `model_name` + `model_version`?
  - **A**:
    - `producer_version` = версия кода компонента (semver/commit), **не** версия модели.
    - В `meta` пишем отдельный блок `models_used` (список), где **на каждую реально вызванную ML-модель** фиксируем:
      - `model_name` (каноническое имя: HF repo id / Triton model name / локальный алиас)
      - `model_version` (строго: HF `revision`/commit sha, Triton `version`, или наш semver/tag)
      - `weights_digest` (sha256/etag/commit) — чтобы различать «одинаковый version, разные веса»
      - `runtime` (`triton` | `inprocess`) + `engine` (`torch`/`onnx`/`tensorrt`) + `precision` (`fp32`/`fp16`) + `device` (`cuda:0`/`cpu`)
    - **Core providers**: пишем все под‑модели, которые они используют (например, `core_clip` → CLIP weights + tokenizer).
    - **Модули**: пишем те модели, которые они вызывают напрямую (если модуль только читает артефакт core provider — у модуля `models_used` может быть пустым, а зависимость фиксируем через `dependencies` в `meta`).
    - **HuggingFace/transformers**: `model_name` = полный путь (`"openai/clip-vit-base-patch32"`), `model_version` = pinned `revision` (commit sha или tag), не “latest”.

- **Q2. Версионирование моделей в Triton**: в `PRODUCTION_ARCHITECTURE.md` указано, что версии моделей фиксируются в `dataprocessor_version` (все run'ы одной версии DataProcessor используют одинаковые версии моделей). Но как это согласуется с возможностью обновления отдельных моделей:
  - если мы обновили только одну модель (например, YOLO с v11.0 на v11.1), нужно ли бампить `dataprocessor_version`?
  - или `dataprocessor_version` бампится только при изменении всего пайплайна, а версии отдельных моделей хранятся в `triton_models.yaml` и прокидываются в `meta` NPZ?
  - **A**:
    - `dataprocessor_version` бампим **только** при изменении кода пайплайна/контрактов/схем (producer/schema/feature extraction), а **не** при апдейте одной модели.
    - Версии моделей живут в `triton_models.yaml` (или в DB-конфиге профиля анализа) и **пинятся на run** (run получает конкретный mapping `component → model:version`).
    - В `meta` NPZ всегда сохраняем **фактический** mapping, использованный в этом run (см. Q1), чтобы воспроизводимость не зависела от “текущего” `triton_models.yaml`.

- **Q3. Совместимость версий моделей при кэшировании**: если у нас есть кэш артефакта от старой версии модели, но мы обновили модель:
  - должны ли мы автоматически инвалидировать кэш (пересчитать артефакт с новой моделью)?
  - или разрешаем использовать старый кэш, но помечаем в `manifest.json`, что артефакт создан со старой версией?
  - как это влияет на idempotency ключ `(platform_id, video_id, run_id, component, config_hash, sampling_policy_version, producer_version, schema_version, model_version*)`?
  - **A**:
    - Idempotency/cache key **обязан включать** `model_signature` (минимум: `model_name + model_version + weights_digest + engine + precision`), иначе кэш станет невалидным при апдейте модели.
    - Базовая политика: **любое изменение `model_signature` = новый ключ = автоматическая “инвалидация”** (старый артефакт остаётся, но не переиспользуется).
    - Опционально (позже): “мягкая совместимость” через `model_compatibility_token`, который явно объявляет, что output совместим (тогда можно разрешить reuse), но это требует явного решения/тестов.
    - В `manifest.json` всегда пишем: `producer_version`, `schema_version`, `model_signature`, `cache_hit` и ссылку на исходный артефакт (если reused).

- **Q4. Версионирование baseline/v1/v2 моделей прогноза**: в `ML_TARGETS_AND_TRAINING.md` описаны три уровня моделей (baseline CatBoost/LightGBM, v1 late-fusion, v2 multimodal transformer). Как версионируем эти модели:
  - отдельные версии для каждого типа (например, `baseline_v1.2.3`, `v1_late_fusion_v2.0.1`, `v2_transformer_v3.1.0`)?
  - или единая версия для всех типов, но с полем `model_type`?
  - как это отражается в inference pipeline и в JSON для фронта?
  - **A**:
    - Версионируем **раздельно по типам**: `baseline_*`, `v1_late_fusion_*`, `v2_transformer_*` (semver + training run id).
    - В inference pipeline всегда явно выбираем `(prediction_model_type, prediction_model_version)` из конфигурации run.
    - В JSON для фронта добавляем блок:
      - `prediction.model.type`, `prediction.model.version`, `prediction.model.run_id`
      - `prediction.model.features_schema_version` (важно для отладки)

#### 2) Развертывание моделей и Triton

- **Q5. Миграция на Triton (план)**: сейчас модели загружаются напрямую в процесс (например, через `torch.load`, `transformers.from_pretrained`, `model_registry`). Когда и как планируем мигрировать на Triton:
  - все модели сразу или поэтапно (сначала самые тяжёлые)?
  - какие модели приоритетны для Triton (GPU-heavy: CLIP, YOLO, emotion_face, scene_classification)?
  - как обеспечим обратную совместимость во время миграции (fallback на прямую загрузку, если Triton недоступен)?
  - **A**: Модели переводим на Triton для каждой глобальной смены моделей, например для baseline нужны 5 моделей, вот их и переводим. Далее baseline обучили, прошли полностью, переходим на v1, где нужны уже 10 моделей их и переводим. Пока никаких Fallback, Triton обязателен.

- **Q6. Конфигурация Triton models (детали)**: в `triton_models.yaml` нужно хранить mapping `component_name → model_name:version`. Но как это работает для:
  - моделей с несколькими вариантами (например, CLIP: `ViT-B/32`, `ViT-L/14`, `RN50` — какой выбираем для `core_clip`)?
  - моделей с параметрами (например, YOLO: `yolo11n.pt`, `yolo11s.pt`, `yolo11x.pt` — выбор зависит от `config.yaml`)?
  - моделей, которые могут быть разными в разных профилях анализа (например, `scene_classification` может использовать `resnet18` или `efficientnet_b0`)?
  - **A**: Выбор моделей будет в ЛК на сайте также как и их настройка. У нас должны быть скомпелированы все модели для всех возможных параметров которые достпны на сайте (какие конкретно определим позже). Как тебе план, давай рекомендации.

- **Q7. Triton batching и динамический batch size**: Triton поддерживает dynamic batching. Как это интегрируется с нашим динамическим батчингом на уровне DataProcessor:
  - Triton сам решает batch size или мы передаём готовый batch?
  - как учитываем ресурсные требования компонентов (`component_resource_requirements` в БД) при формировании batch для Triton?
  - нужно ли конфигурировать `max_batch_size` в Triton model config отдельно для каждого компонента?
  - **A**: У нас своя система батчирования основаная больше на оптимизации памяти. Наши батчи мы передаем в Triton (они уже оптимальны для доступной памяти). Остальное реши сам.

- **Q8. Triton health checks и fallback**: если Triton недоступен или модель не загружена:
  - должны ли компоненты автоматически fallback на прямую загрузку модели (как сейчас)?
  - или это считается ошибкой и run должен упасть (согласно no-fallback policy)?
  - как различаем "Triton временно недоступен" (retry) от "модель не найдена" (fail-fast)?
  - **A**: Triton будет запускаться локально и должен быть достпуен всегда. Никаких fallback.

- **Q9. Triton model repository и обновления**: как обновляем модели в Triton:
  - через CI/CD при деплое DataProcessor (автоматически)?
  - вручную через админку/API (для hot-fix)?
  - как обеспечиваем zero-downtime обновление (rolling update, A/B тестирование)?
  - нужно ли версионировать model repository (например, `models/emotion_net/v1.2/` vs `models/emotion_net/v1.3/`)?
  - **A**:
    - Обновление моделей: **через CI/CD** (основной путь) + **ручной hot-fix** (ограниченно, с аудит‑логом).
    - Model repository **версионируем папками**: `models/<model_name>/<version>/...` + отдельный `config.pbtxt`/metadata per version.
    - Zero-downtime: держим **старую и новую версии параллельно**, переключение происходит на уровне `triton_models.yaml`/DB профиля (A/B по пользователям/профилям возможно).
    - После прогрева/валидации новую версию делаем default, старую — оставляем на “grace period”, потом архивируем.

#### 3) Загрузка, кэширование и управление моделями

- **Q10. Model registry и singleton паттерн**: в `TextProcessor/src/core/model_registry.py` используется глобальный registry с кэшированием моделей по ключу `(model_name, device, fp16)`. Нужно ли распространить этот паттерн на все процессоры:
  - единый глобальный registry для всех моделей (Visual/Audio/Text)?
  - или отдельные registry для каждого процессора?
  - как управляем жизненным циклом моделей (когда выгружаем из памяти, при нехватке GPU memory)?
  - **A**: registry для каждого процессора. Остальное сам реши.

- **Q11. Preloading моделей при старте**: нужно ли предзагружать модели при старте worker'а:
  - все модели сразу (быстрее первый запрос, но больше memory)?
  - только Tier-0 модели (required для prediction)?
  - lazy loading по требованию (как сейчас)?
  - как это влияет на health check `/health/ready` (готов ли worker принимать запросы)?
  - **A**:
    - Preload: **только Tier‑0** (то, без чего нельзя начать run/сделать прогноз/критичные core providers).
    - Остальное: lazy loading по требованию.
    - `/health/ready` = OK только если:
      - Triton доступен (если обязателен для данного сервиса)
      - Tier‑0 модели загружены/доступны
      - (желательно) прошёл быстрый sanity‑inference для Tier‑0.

- **Q12. Кэширование весов моделей на диске**: модели из HuggingFace/transformers скачиваются в кэш (например, `~/.cache/huggingface/`). Как управляем этим кэшем:
  - общий кэш для всех worker'ов (shared volume в MinIO/S3)?
  - локальный кэш на каждом worker'е (быстрее, но дублирование)?
  - версионирование кэша (если обновили модель, нужно ли очищать старый кэш)?
  - **A**:
    - Стратегия: **локальный кэш на воркере** (скорость) + возможность подключить **shared read-only volume** (ускорение cold start в кластере).
    - Обязательное правило: всегда грузим по pinned `revision`, значит кэш **естественно версионируется** по ревизии.
    - Очистка: по лимиту размера/TTL (LRU на диске). Старые ревизии можно удалять, если они не используются активными версиями профилей.

- **Q13. GPU memory management и OOM prevention**: как предотвращаем OOM при загрузке нескольких моделей:
  - лимиты на количество одновременно загруженных моделей?
  - приоритизация моделей (Tier-0 загружаем первыми, Tier-1 по требованию)?
  - автоматическая выгрузка неиспользуемых моделей (LRU eviction)?
  - как это согласуется с динамическим батчингом (если освободили память для batch, но модель выгрузили)?
  - **A**:
    - Основа: “чеклист памяти” (как ты описал) + runtime‑проверка `torch.cuda.mem_get_info()`/NVML.
    - Политика:
      - не загружаем модель, если по оценке не влезаем (fail-fast с понятной ошибкой `insufficient_gpu_memory`)
      - Tier‑1 модели могут выгружаться (LRU), Tier‑0 — pinned
      - ограничение `max_loaded_models_per_gpu` (конфиг)
    - Для батчинга: выбирать batch_size **консервативно** до старта, избегая OOM‑“проб”.

- **Q14. FP16/FP32 precision и совместимость**: в коде есть поддержка FP16 (например, в `TextProcessor`). Как стандартизируем:
  - все модели по умолчанию FP16 на GPU (экономия памяти) или FP32 (точность)?
  - можно ли настраивать precision per model в конфиге?
  - как это влияет на воспроизводимость (FP16 может давать разные результаты на разных GPU)?
  - нужно ли фиксировать precision в `meta` NPZ?
  - **A**:
    - Default:
      - GPU: **FP16**, если модель валидирована (качество/стабильность), иначе FP32.
      - CPU: FP32 (если вообще используется).
    - Precision **настраивается per model** в конфиге run.
    - Для воспроизводимости в `meta` фиксируем `precision`, `engine`, `device` и (по возможности) `cuda/cuDNN` версии.
    - Детерминизм: фиксируем seed’ы и включаем детерминистичные режимы где возможно, но допускаем малые численные расхождения между GPU/engine.

#### 4) Конфигурация моделей и параметры

- **Q15. Конфигурация моделей в config.yaml vs профиле анализа**: сейчас параметры моделей (например, `model_arch`, `batch_size`, `device`) хранятся в `VisualProcessor/config.yaml`. Но в проде пользователь выбирает профиль анализа. Как это согласуется:
  - параметры моделей фиксируются в `config_hash` (часть профиля)?
  - или параметры моделей глобальные для всех профилей, а профиль только включает/выключает компоненты?
  - можно ли пользователю настраивать параметры моделей (например, "быстрый режим" = меньший batch_size)?
  - **A**: config.yaml грубо говоря формируеться для каждого нового анализа из того что выбрал пользователь. Остально реши сам.

- **Q16. Выбор вариантов моделей (например, CLIP ViT-B/32 vs ViT-L/14)**: разные варианты одной модели имеют разное качество/скорость. Как выбираем:
  - фиксируем один вариант для всех run'ов (например, `ViT-B/32` для baseline)?
  - разрешаем выбирать в профиле анализа (например, "быстрый" = `ViT-B/32`, "качественный" = `ViT-L/14`)?
  - как это влияет на кэширование (разные варианты = разные артефакты)?
  - **A**: разрешаем выбирать в профиле анализа. с кэшем реши сам.

- **Q17. Batch size и адаптация под ресурсы**: в коде есть логика `auto_batch_size()` для core providers. Как стандартизируем:
  - все компоненты должны поддерживать динамический batch_size?
  - как учитываем доступную GPU memory при выборе batch_size (проверка перед запуском или адаптация во время выполнения)?
  - нужно ли фиксировать использованный batch_size в `meta` NPZ для воспроизводимости?
  - **A**:
    - Да: все компоненты, где это даёт выигрыш, поддерживают batch processing.
    - Выбор batch_size: **перед запуском компонента** на основе:
      - доступной GPU памяти (runtime)
      - чеклиста (оценка памяти/времени)
      - размера кадра/сегмента и числа потоков
    - Использованный batch_size **фиксируем в `meta`** (важно для воспроизводимости и профайлинга).

- **Q18. Device selection (CPU/GPU/auto)**: в конфигах есть параметр `device: "cuda"` или `device: "auto"`. Как это работает в проде:
  - `device: "auto"` всегда выбирает GPU, если доступен, или может fallback на CPU?
  - можно ли указывать конкретный GPU (например, `device: "cuda:1"`) для multi-GPU setup?
  - как это влияет на кэширование (артефакт, созданный на CPU, совместим с артефактом на GPU)?
  - **A**:
    - `device: auto` = выбираем GPU, **если компонент помечен как GPU-required**. CPU используется только там, где GPU “не нужен по дизайну”.
    - Поддерживаем явный выбор: `cuda:0`, `cuda:1`, ... + планировщик распределения по GPU.
    - Для кэша/воспроизводимости: фиксируем `device` и `precision` в `meta`; и включаем `device_type` в `model_signature` (CPU/GPU считаем разными окружениями).

#### 5) Core providers vs modules (архитектура моделей)

- **Q19. Миграция модулей на core providers**: в документации описано, что модули постепенно переходят на использование core providers (например, `behavioral` использует `core_face_landmarks`). Как управляем этим переходом:
  - все модули обязаны использовать core providers, если они доступны (no-fallback)?
  - или модули могут иметь fallback на локальную инициализацию модели (как сейчас в `behavioral`)?
  - как версионируем core providers отдельно от модулей (если обновили `core_clip`, нужно ли пересчитывать все артефакты модулей, которые его используют)?
  - **A**:
    - Политика: если модуль требует данные, которые предоставляет core provider — **модуль обязан использовать core provider, без fallback**.
    - Версионирование:
      - core provider имеет свой `producer_version` и `models_used`.
      - модуль, который зависит от core provider, фиксирует в `meta.dependencies` сигнатуры входных артефактов (включая `producer_version/schema_version/model_signature` core provider).
      - Таким образом, апдейт `core_*` автоматически ведёт к новому ключу кэша downstream‑модулей.

- **Q20. Зависимости между core providers и модулями**: если модуль зависит от core provider (например, `video_pacing` зависит от `core_clip` и `core_optical_flow`):
  - модуль должен проверять наличие артефакта core provider перед запуском (fail-fast)?
  - или core provider всегда запускается первым (гарантированно в DAG)?
  - что делать, если core provider вернул `status="empty"` (например, `core_face_landmarks` на видео без лиц) — модуль тоже должен вернуть empty или может работать с частичными данными?
  - **A**: 1. модуль должен проверять наличие артефакта. 2. если core provider вернул `status="empty"`, то модуль также возвращает `status="empty"` для тех полей, которым нужны данные этого провайдера, но по остальным полям делает анализ (с частичными данными). Этот вопрос должен быть чатильно рассмотрен при планировке качественого обучения моделей, так как логично что далеко не на всех видео будут все данные, например не на всех видео будут лица.

- **Q21. Core providers в отдельных venv**: в документации упоминается, что `core_face_landmarks` может требовать изолированную venv из-за конфликтов Mediapipe. Как стандартизируем:
  - все core providers в отдельных venv или только проблемные?
  - как управляем зависимостями (requirements.txt per core provider)?
  - как это влияет на deployment (Docker образы per core provider или один образ с несколькими venv)?
  - **A**:
    - Сейчас: отдельные venv **только для проблемных** (как `core_face_landmarks`), остальные — общая среда.
    - Стандартизация: для каждого “изолированного” core provider держим свой `requirements-<provider>.txt` + версию окружения фиксируем в `meta.runtime_env` (например, hash lockfile).
    - Deployment (практично для MVP): один Docker‑образ + несколько venv внутри; если конфликтов станет много — переходим на “образ на provider”.

#### 6) Модели для обучения (baseline/v1/v2)

- **Q22. Feature extraction для обучения vs inference**: модели обучения (baseline/v1/v2) используют фичи из NPZ артефактов. Как обеспечиваем совместимость:
  - фичи для обучения должны быть идентичны фичам для inference (те же агрегаты, те же схемы)?
  - что делать, если схема фичи изменилась (например, добавили новое поле в `core_clip` embeddings) — нужно ли переобучать модель?
  - как версионируем feature schemas отдельно от моделей (например, `feature_schema_v1.2` → `baseline_model_v2.0`)?
  - **A**:
    - Training и inference используют **одну и ту же библиотеку feature extraction** (общий код/спеки).
    - Вводим `feature_schema_version` (semver) и `feature_extractor_version` (commit/semver) и пишем их в:
      - артефакт фичей (NPZ/Parquet)
      - `manifest.json`
      - метаданные модели прогноза
    - При изменении схемы фичей:
      - additive change (добавили поле) возможно без retrain только если модель его игнорирует (но лучше retrain)
      - breaking change (убрали/переименовали/поменяли смысл) = **обязательный retrain**

- **Q23. Multi-horizon и multi-target модели**: в `ML_TARGETS_AND_TRAINING.md` описаны multi-target (views+likes) и multi-horizon (7d/14d/21d). Как это реализуется:
  - одна модель с несколькими heads (по горизонту и таргету)?
  - отдельные модели для каждого горизонта (например, `baseline_14d`, `baseline_21d`)?
  - как обрабатываем missing targets (7d может отсутствовать) — mask в loss или отдельная модель?
  - **A**:
    - Для v2/v1: **одна модель** с multi-head выходом на 6 значений: `(views_7d, views_14d, views_21d, likes_7d, likes_14d, likes_21d)`.
    - Missing targets: masked loss (для отсутствующих таргетов вес = 0), чтобы не плодить модели.
    - Для baseline (CatBoost/LightGBM): целимся в **1 модель на таргет с multi-output по горизонтам** (или 2 модели: views и likes), а не 6 отдельных. Если библиотечно будет сложно — допускаем 6 маленьких baseline‑моделей, но это “план Б”.

- **Q24. Cold-start и возраст видео**: в документации упоминается `video_age_hours_at_snapshot1` как обязательная фича. Как это влияет на модели:
  - одна универсальная модель для всех возрастов (как указано в MVP)?
  - или отдельные модели для cold-start (<24h) и зрелых видео (>30d)?
  - как учитываем возраст в inference (пользователь загружает видео сразу после публикации vs через неделю)?
  - **A**:
    - MVP: **одна универсальная модель**, но `video_age_hours_at_snapshot1` + derived features (лог‑возраст, age bucket, взаимодействия) — обязательны.
    - В inference: возраст идёт как фича + влияет на **confidence/интервал** (обычно чем моложе — тем шире неопределённость).
    - Если позже увидим сильную деградацию на cold-start — добавим второй специализированный вариант модели (cold vs mature) и будем выбирать по age‑threshold.

- **Q25. Baseline как sanity-check**: в документации baseline (CatBoost/LightGBM) должен оставаться как контрольная точка. Как это работает в проде:
  - baseline всегда запускается параллельно с v2 и сравниваются результаты?
  - или baseline запускается только при ошибке v2 (fallback)?
  - как часто переобучаем baseline (при каждом обновлении фичей или только при значительных изменениях)?
  - **A**:
    - Baseline в проде используем как **shadow/sanity**:
      - на части трафика (например, 1–5%) считаем baseline параллельно и логируем расхождения
      - для full‑traffic считаем baseline периодически/батчами для мониторинга дрейфа
    - Fallback для prediction: допускаем **только как degraded‑mode** (если v2 inference упал), с явным флагом `prediction_status="degraded"` в JSON.
    - Retrain baseline: как ты сказал — при изменении фичей/логики/схем + периодически (например, раз в N недель) для актуализации.

#### 7) Inference pipeline и модели прогноза

- **Q26. Загрузка обученных моделей в inference**: где храним артефакты обученных моделей (baseline/v1/v2):
  - в репозитории DataProcessor (например, `Training/artifacts/<run_id>/`)?
  - в MinIO/S3 (отдельный bucket для моделей)?
  - в Triton (как serving models)?
  - как версионируем и выбираем, какую версию модели использовать для inference?
  - **A**:
    - Храним артефакты в HuggingFace (как ты планируешь), но **пинним на commit/tag**.
    - Рекомендуется mirror в S3/MinIO (опционально) для устойчивости.
    - Выбор версии для inference:
      - `prediction_model_type` + `prediction_model_version` в конфиге run/профиле
      - дефолт = “latest stable” (обновляется через релиз-процесс, не руками в коде)

- **Q27. Feature extraction в inference**: в `Inference/extract_features.py` нужно построить feature vector из NPZ артефактов. Как обеспечиваем:
  - те же агрегаты, что и в training (mean, max, min, std, etc.)?
  - обработка missing values (NaN → 0 или median, как в training)?
  - валидация feature vector (проверка, что все required фичи присутствуют)?
  - **A**:
    - “Single source of truth”: `feature_spec.yaml` (список фичей + агрегаты + нормализация/импутация) используется и в training, и в inference.
    - Missing values:
      - если модель умеет NaN (деревья) — оставляем NaN
      - иначе: импутация как в training + (желательно) дополнительные mask‑фичи “было ли значение”
    - Валидация:
      - проверяем наличие required артефактов/полей
      - если critical missing → fail-fast с понятной ошибкой
      - если optional missing → заполняем по правилам spec и пишем warning в manifest

- **Q28. Multi-model inference (baseline + v1 + v2)**: если в проде доступны несколько моделей (baseline, v1, v2), как выбираем:
  - всегда используем v2, fallback на v1/baseline при ошибке?
  - ensemble (усреднение прогнозов от всех моделей)?
  - A/B тестирование (часть пользователей получают v2, часть — v1)?
  - как это отражается в JSON для фронта (какая модель использовалась)?
  - **A**:
    - Default: используем **самую новую stable** модель (обычно v2).
    - Роллаут: A/B тест (bucket по user_id/video_id) между v2 и v1 (или v2 и baseline), результаты логируем.
    - Ensemble: не в MVP; добавим позже только если даст значимый выигрыш и не усложнит latency/объяснимость.
    - В JSON для фронта всегда пишем:
      - `prediction.model.*` (тип/версия)
      - `prediction.experiment_bucket` (если A/B)
      - `prediction_status` (`ok` | `degraded`)

- **Q29. Confidence и uncertainty estimation**: модели прогноза должны возвращать не только значение, но и confidence. Как это реализуем:
  - для baseline (CatBoost/LightGBM) — используем prediction intervals или просто фиксированное значение?
  - для v2 (transformer) — можно ли добавить uncertainty head (например, variational dropout)?
  - как отображаем confidence в UI (диапазон значений, визуализация неопределённости)?
  - **A**:
    - Baseline: prediction interval через
      - quantile regression (если поддержим), или
      - conformal prediction на валидации (отдельно по горизонтам/age buckets).
    - v2: heteroscedastic head (mean + log-variance) или MC‑dropout (дороже по latency; скорее не MVP).
    - В UI: показываем **диапазон** (P10–P90 или 95% CI) + `confidence_score` (0..1) и визуализацию “полосы неопределённости”.

#### 8) Модели в разных процессорах (Visual/Audio/Text)

- **Q30. Единообразие загрузки моделей**: в `TextProcessor` используется `model_registry`, в `AudioProcessor` модели загружаются напрямую, в `VisualProcessor` — через core providers или напрямую. Нужно ли стандартизировать:
  - единый паттерн загрузки для всех процессоров?
  - или каждый процессор может использовать свой подход (но с общими правилами версионирования)?
  - **A**:
    - Да, стандартизируем через общий контракт `ModelProvider`:
      - единый формат `model_signature`/логирования/метрик
      - единый API “get_model()/infer()”
      - единая запись `models_used` в `meta`
    - Реализация поэтапно: сначала Visual → Audio → Text, без переписывания всего сразу.

- **Q31. AudioProcessor модели (CLAP, SpeechBrain, etc.)**: в `AudioProcessor` используются модели CLAP, SpeechBrain, Essentia. Как управляем:
  - версионирование этих моделей (они не из HuggingFace, а из отдельных репозиториев)?
  - fallback между моделями (например, если CLAP недоступен, используем более простой алгоритм) — это нарушает no-fallback policy?
  - как это отражается в `meta` NPZ (какая модель использовалась)?
  - **A**:
    - Версионирование как в Q1: `model_name`, `model_version`, `weights_digest`, `engine`, `precision`, `device`.
    - No-fallback: если выбранная модель недоступна — компонент падает (fail-fast) и run помечается error.
    - Да, Triton используем и для Audio/Text, если модели подходят под serving (GPU/батчинг/стандартизируем входы).

- **Q32. TextProcessor embeddings models**: в `TextProcessor` используются SentenceTransformer модели (например, `intfloat/multilingual-e5-large`). Как управляем:
  - кэширование embeddings (уже есть в коде через `_cache_path_vector`) — это разрешено или нужно отключить для воспроизводимости?
  - можно ли использовать разные модели для разных языков (например, русскоязычные видео — одна модель, англоязычные — другая)?
  - как это влияет на feature schema (размерность embeddings может отличаться)?
  - **A**:
    - Кэш embeddings разрешён **только** если ключ = `hash(text + model_signature + preprocessing_flags)` и результат детерминирован.
    - Для MVP лучше одна **мультиязычная** embedding‑модель (фиксированная размерность) → проще schema.
    - Если захотим разные модели по языкам:
      - либо требуем одинаковую размерность
      - либо вводим разные фичи/схемы и версионируем `feature_schema_version` (усложняет пайплайн).

#### 9) Мониторинг и observability моделей

- **Q33. Метрики производительности моделей**: что собираем для мониторинга моделей:
  - время загрузки модели (cold start)?
  - время inference per batch/per frame?
  - GPU memory usage per model?
  - throughput (frames/second, videos/hour)?
  - где храним эти метрики (Prometheus, manifest.json, БД)?
  - **A**:
    - Собираем (минимум):
      - `model_load_seconds` (cold start)
      - `inference_seconds` (histogram) per component/model_version
      - `gpu_memory_bytes` (used/free) + `max_memory_bytes` per worker
      - `throughput` (frames/s, videos/h)
      - `cache_hit_rate` (артефакты + embeddings)
      - `error_rate`/`oom_rate` per component
    - Хранение:
      - realtime: Prometheus (+ Grafana)
      - per-run: `manifest.json` (для дебага) + агрегаты в БД (для аналитики).

- **Q34. Логирование использования моделей**: нужно ли логировать:
  - какая модель использовалась для каждого компонента (в `manifest.json` уже есть `producer_version`, но нужно ли `model_name`/`model_version`)?
  - параметры модели (batch_size, device, precision)?
  - warnings/errors при загрузке или использовании модели?
  - **A**:
    - Да, логируем для каждого компонента:
      - `model_signature` (см. Q1)
      - параметры инференса (batch_size, device, precision, engine)
      - события загрузки (load start/ok/fail), warnings, retries
    - В `manifest.json` фиксируем и факты, и итоги (latency, cache hit, warnings).

- **Q35. Алерты для моделей**: на что алертим:
  - модель не загрузилась при старте worker'а?
  - время inference выросло значительно (например, >2x от baseline)?
  - частота OOM при использовании модели (например, >10% runs)?
  - Triton недоступен или модель не найдена?
  - **A**:
    - Triton недоступен / модель не найдена / модель не загружается.
    - Error rate по компоненту > порога (например, 1–2% за 5–10 минут).
    - Latency p95/p99 выросла >2x от baseline окна.
    - OOM rate > порога (например, >1% runs) или GPU free memory < порога длительно.
    - Очередь задач/lag (если есть брокер) растёт.

#### 10) Обработка ошибок и edge cases

- **Q36. Ошибки загрузки моделей**: если модель не загрузилась (например, файл повреждён, недостаточно памяти):
  - компонент должен упасть с ошибкой (fail-fast)?
  - или есть retry логика (например, попытка загрузить модель 2-3 раза)?
  - как это отражается в `manifest.json` (status="error", error="model_load_failed")?
  - **A**:
    - Fail-fast на уровне компонента/run.
    - Внутренний retry допускаем **только** для transient ошибок IO/сети (например, 3 попытки с backoff).
    - `manifest.json`:
      - `status="error"`
      - `error_code` (например, `model_load_failed`, `insufficient_gpu_memory`, `triton_unavailable`)
      - `error_message`, `retry_count`, `stacktrace` (опционально, но полезно).

- **Q37. Несовместимость версий моделей**: если артефакт был создан со старой версией модели, но мы обновили модель:
  - можем ли мы использовать старый артефакт с новой моделью (если schema совместима)?
  - или нужно пересчитать артефакт (инвалидировать кэш)?
  - как проверяем совместимость (schema validation)?
  - **A**:
    - По умолчанию: новая `model_signature` = новый cache key (то есть артефакт не reused).
    - Reuse между версиями разрешаем **только** если:
      - `schema_version` совпадает, и
      - `model_compatibility_token` совпадает (явно выставляется нами после тестов), и
      - есть regression tests, подтверждающие эквивалентность/допуск.
    - Schema validation: валидируем NPZ по `schema_version` (jsonschema/pydantic) + проверяем обязательные поля/shape.

- **Q38. Модели для edge cases (видео без лиц, без звука, etc.)**: как работают модели на edge cases:
  - `emotion_face` на видео без лиц — модель должна вернуть `status="empty"` или может работать с пустым входом?
  - `scene_classification` на очень коротком видео (<5 секунд) — достаточно ли кадров для классификации?
  - как обрабатываем частичные данные (например, только первые 10 минут из 20-минутного видео)?
  - **A**:
    - Видео без лиц:
      - `core_face_*` возвращает `status="empty"` + пустые arrays/нулевые counts; модуль `emotion_face` также `status="empty"` по зависимым полям.
    - Видео без звука:
      - `core_audio_*`/audio‑модули: `status="empty"` и warnings в manifest.
    - Очень короткое видео (~5 сек):
      - если недостаточно кадров для модели/агрегатов → `status="empty"` или `status="degraded"` (если частичный результат возможен) + явный warning.
    - Частичная обработка (например, только первые 10 минут):
      - допускается как sampling policy (uniform/scene-aware), но в `meta` обязательно фиксируем `coverage` (какая доля/таймспаны обработаны).

#### 11) Батчинг и оптимизация

- **Q39. Batch processing внутри компонентов**: многие компоненты обрабатывают кадры батчами. Как стандартизируем:
  - все компоненты должны поддерживать батчинг?
  - как выбираем оптимальный batch_size (статический из конфига или динамический на основе доступной памяти)?
  - нужно ли фиксировать использованный batch_size в `meta` NPZ?
  - **A**: Я думаю что все компоненты могут поддреживать батчинг и должны это делать. Про динамический батчинг уже не раз говорил - опираемся на достпную память и заранее подготовленый чек-лист в котором указанно какие компоненты и при каких входных размерах кадра (сегмента) сколько требуют памяти

- **Q40. Cross-video batching**: можно ли обрабатывать несколько видео параллельно (например, батч из 4 видео для `scene_classification`):
  - это разрешено или каждый run обрабатывается отдельно?
  - как это влияет на кэширование (артефакты per video, но модель общая)?
  - как управляем GPU memory при cross-video batching?
  - **A**: Это также относиться к нашему динамическому батчингу. То есть изначально каждый модуль поддерживает параллельность внутри одного видео и разбиваем (батчим) видео мы исходя из нашей логики динамического батчинга. Тут нужно разработать умное расспаралелвание (умный батчинг) которое само будет решать на каких уовнях ему разбить видео и стоит ли это делать между видео основываясь на доступной памяти и чек-листе. 
  Пример: поступают на вход 2 видео с разрешением HD и задача обработать их на 2 модулях, например object_detection и scene_classification. Умная система запрашивает доступную память, например 16 гб, далее смотрит в чек-лист и видит сколько памяти потребует каждый модуль при обработке одного кадра в HD, например object_detection - 1 гб и время 7 секунд, scene_classification - 1.5 гб и время 11 секунд. Далее учитывая то что кадры обрабатываються независимо и не происходим OOM по памяти (тоесть она скидываеться от кадра к кадру), умная система решает, сколько паралельных кадров она может запустить в каждом модуле и стоит ли запускать эти два видео параллельно, например Вариант 1 - она решает что быстрее будет запускать по 3 параллельных кадра на object_detection и 2 параллельных кадра на scene_classification (в сумме 6 гб памяти будет рассходоваться), при этом она запускает два этих видео параллельно (получаеться если видео равны по разрешению будет 12 гб памяти рассходоваться). Или же она может выбрать Вариант 2 - не запускать видео параллельно но при этом запустить больше параллельных кадров, например 6 паралл. кадров на object_detection и 4 на scene_classification, в сумме те же 12гб, но время уже разное в сравнении с первым вариантом. Так вот суть этой умной системы это понять какой вариант будет быстрее и оптимальнее, опираясь на следующие вещи: Кол-во входящих видео, разрешение видео, кол-во кадров, доступная память (причем как GPU так и CPU RAM), чек-лист (память и время на еденицу обработки). Это я сейчас описал простой случай с двумя вариантами, но у нас в системе будет несколько уровней батчинга (как минимум Triton batching это уже еще один уровень), кадры не всегда независимы, их кол-во разное, разрешение разное. Соответственного эта умная система должны стоять выше всего (выше Fetcher, DataProcessor, Models). Это большая и сложная система, тут нужен четкий план по ее разработке и внедрению. (В рассказе я описывал кадры как еденицу обработки но в финальном варианте это может быть не так, за еденицу обработки могут быть приняты сегменты видео состоящие из нескольких кадров, но для чек-листа я считаю правильным брать за еденицу именно кадры и потом если нужно рассчитывать это все для сегментов)

- **Q41. Оптимизация inference (ONNX, TensorRT, etc.)**: планируем ли оптимизацию моделей:
  - конвертация в ONNX для ускорения?
  - TensorRT для NVIDIA GPU?
  - quantization (INT8) для экономии памяти?
  - как это влияет на воспроизводимость (оптимизированная модель может давать немного другие результаты)?
  - **A**:
    - Да: ONNX + TensorRT там, где это даёт прирост. Quantization минимум до FP16 (без INT8 в MVP).
    - Важный момент: оптимизированные движки **могут** давать немного другие числа (другие kernels/порядок операций).
    - Поэтому `engine`/`engine_build_hash` включаем в `model_signature` → это “другая версия” для кэша и воспроизводимости.

#### 12) Безопасность и лицензии моделей

- **Q42. Лицензии моделей**: используемые модели имеют разные лицензии (MIT, Apache 2.0, проприетарные). Как управляем:
  - нужно ли документировать лицензии всех моделей (для compliance)?
  - можно ли использовать проприетарные модели в проде (есть ли ограничения)?
  - как это влияет на распространение продукта (open-source vs коммерческий)?
  - **A**:
    - Ведём инвентарь `MODEL_LICENSES.md` (model_name, source, license, link, ограничения).
    - CI‑проверка: нельзя подтягивать модели без лицензии/с несовместимой лицензией для коммерции.
    - Проприетарные модели можно, если условия явно разрешают коммерческое использование; храним доказательства/ссылки.

- **Q43. Безопасность моделей (adversarial attacks, data poisoning)**: как защищаемся:
  - валидация входных данных перед inference (проверка формата, размера, диапазона значений)?
  - мониторинг аномальных результатов (например, очень высокий confidence на заведомо неправильном входе)?
  - нужно ли тестировать модели на adversarial examples?
  - **A**: Да, все тесты. Нам важно качество результата.

#### 13) Тестирование моделей

- **Q44. Unit тесты для моделей**: нужно ли тестировать:
  - загрузку моделей (проверка, что модель загружается без ошибок)?
  - inference на тестовых данных (проверка, что модель возвращает валидный output)?
  - совместимость версий (старый артефакт + новая модель)?
  - **A**: Да

- **Q45. Integration тесты с моделями**: как тестируем:
  - полный пайплайн с реальными моделями (может быть медленно)?
  - mock моделей для быстрых тестов (но тогда не проверяем реальную работу)?
  - golden tests (сохраняем ожидаемые результаты и сравниваем)?
  - **A**:
    - Быстрый контур (каждый PR): mocks + маленькие фикстуры (минимальные входы, проверка формата/схем).
    - Ночной/периодический (CI nightly): e2e пайплайн с реальными моделями на 2–3 маленьких видео.
    - Golden tests: для критичных компонентов держим “эталон” outputs (или агрегаты/метрики), допускаем tolerances для float.

- **Q46. Performance тесты моделей**: нужно ли тестировать:
  - время inference на разных размерах входных данных?
  - memory usage при разных batch sizes?
  - throughput под нагрузкой?
  - как часто запускаем (на каждом PR или периодически)?
  - **A**:
    - Да: latency/memory/throughput.
    - Частота:
      - lightweight microbench на PR (только изменённые компоненты/модели)
      - полный нагрузочный прогон nightly/weekly.
    - Вводим budgets (p95 latency, max memory) и алертим/блокируем релиз при регрессии.

---

### Round 2 — вопросы (следующие уточнения перед реализацией)

#### 1) Кэш, совместимость и ключи

- **Q1. Политика “model_compatibility_token”**: хотим ли мы сразу вводить совместимость между версиями модели (reuse артефактов при апдейте), или в MVP фиксируем правило “новая модель = новый кэш” без исключений?
  - **A**: в MVP фиксируем правило “новая модель = новый кэш” без исключений

- **Q2. Детерминизм**: какой допуск по воспроизводимости считаем нормой (например, “строго одинаково” vs “допускаем небольшие fp16/engine расхождения”)?
  - **A**: допускаем небольшие fp16/engine расхождения, но:
    - на одном и том же железе/engine/precision должны получать “практически одинаковый” результат (детерминизм best-effort)
    - для кросс-окружений (другая GPU/другой engine) считаем это другой версией (`model_signature` отличается), поэтому сравнение делаем с tolerances
    - в `meta/manifest` фиксируем: seed, engine, precision, device, версии CUDA/cuDNN (если применимо)

- **Q3. Храним ли старые артефакты**: сколько времени/версий артефактов держим (TTL/GC), и можно ли удалять всё, что не относится к “последним N версиям моделей/профилей”?
  - **A**: да, делаем GC:
    - hard cap: `hard_cap_days = 60` (согласовано с `docs/PRIVACY_AND_RETENTION.md`)
    - держим минимум: последний успешный `run_id` на ключ `(platform_id, video_id, config_hash, sampling_policy_version, model_signature-set)` + N=2 предыдущих (для дебага)
    - промежуточные/временные файлы (frames_dir и т.п.) — короткий TTL (по текущим докам 7 дней)

#### 2) Triton и профили анализа

- **Q4. Где живёт mapping `component → model:version` в проде**: только `triton_models.yaml` в репо, или это будет сущность в БД (привязка к профилю анализа/плану подписки)?
  - **A**: полуфинал для MVP:
    - source-of-truth = **профиль анализа в БД** (пользователь/тариф выбирает профиль → он содержит mapping и параметры)
    - но допускаем “bootstrap” через `triton_models.yaml` в репо (дефолтные значения), пока нет UI/админки
    - критично: для каждого run мы сохраняем **resolved mapping** в `manifest.json`/`meta` (см. Round 1), чтобы воспроизводимость не зависела от “текущих настроек”

- **Q5. Модельные параметры в UI**: какие параметры пользователь может настраивать в ЛК (пример: “fast/quality”, выбор варианта CLIP/YOLO), а какие жёстко фиксируем бэком (max batch, precision, engine)?
  - **A**: Решим после обучения первых моделей

#### 3) Prediction (baseline/v1/v2)

- **Q6. Fallback для прогноза**: если v2 prediction упал (не фича‑экстракция, а именно inference модели прогноза), возвращаем ли baseline как degraded‑mode, или считаем это ошибкой и показываем пользователю “prediction unavailable”?
  - **A**: делаем гибрид:
    - по умолчанию: **авто-degraded** → если primary (v2) упал, пытаемся v1, затем baseline; если baseline ок — отдаём результат с `prediction_status="degraded"`
    - в UI: даём пользователю кнопку “Re-run prediction with another model” (если доступно по тарифу) — это отдельная операция, может стоить доп. кредиты/время

- **Q7. Интервалы уверенности**: какой формат отдаём в API/UI по умолчанию:
  - вариант A: один `confidence_score` (0..1)
  - вариант B: интервал (p10/p90) + score
  - вариант C: несколько интервалов (50/80/95%)
  - **A**: отдаём все варианты, но с чётким дефолтом:
    - default в API/UI: `confidence_score` + интервал p10/p90
    - дополнительно (если посчитано): интервалы p50 (median), p05/p95 и p025/p975

#### 4) Text embeddings и языки

- **Q8. MVP по embeddings**: фиксируем одну мультиязычную модель навсегда для MVP, или допускаем позже “RU‑model + EN‑model” с усложнением схемы?
  - **A**: допускаем позже “RU‑model + EN‑model” с усложнением схемы

#### 5) Observability

- **Q9. Где ты хочешь смотреть метрики**: достаточно Grafana/Prometheus + логи, или нужен ещё отдельный “ML observability” слой (таблица в БД с метриками по каждому run + UI в админке)?
  - **A**: второй вариант звучит лучше

#### 6) Multi-GPU scheduling

- **Q10. Политика распределения по GPU**:
  - вариант A: “один run → один GPU” (проще)
  - вариант B: “компоненты run’а могут идти на разных GPU” (сложнее, но эффективнее)
  - **A**: вариант B

---

