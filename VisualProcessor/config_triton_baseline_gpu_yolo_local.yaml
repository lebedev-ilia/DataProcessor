core_providers:
  core_clip: true
  core_face_landmarks: false
  core_depth_midas: true
  core_object_detections: true
  core_optical_flow: true

modules:
  cut_detection: false
  scene_classification: false
  face_detection: false
  detalize_face_modules: false
  emotion_face: false
  behavioral: false
  optical_flow: false
  action_recognition: false
  color_light: false
  frames_composition: false
  shot_quality: false
  video_pacing: false
  story_structure: false
  micro_emotion: false
  similarity_metrics: false
  text_scoring: false
  uniqueness: false

global:
  root_path: null
  frames_dir: null
  rs_path: null
  max_parallel_modules: 1
  gpu_max_concurrent: "auto"

core_clip:
  runtime: "triton"
  batch_size: 1
  triton_preprocess_preset: "openai_clip_224"

core_depth_midas:
  runtime: "triton"
  batch_size: 1
  triton_preprocess_preset: "midas_256"
  out_width: 256
  out_height: 256

core_optical_flow:
  runtime: "triton"
  triton_preprocess_preset: "raft_256"

core_object_detections:
  # New Triton runtime path: YOLO inference in Triton, NMS+ByteTrack in-process.
  runtime: "triton"
  batch_size: 1
  # Local weights path only used for class-names (NO downloads). Supports env expansion.
  model: "${DP_MODELS_ROOT}/visual/yolo/yolo11x.pt"
  box_threshold: 0.6
  iou_threshold: 0.3
  # default branch
  triton_preprocess_preset: "yolo11x_640"


