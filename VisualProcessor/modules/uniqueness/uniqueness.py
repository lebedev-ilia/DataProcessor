"""
# ‚≠ê 9. –°—Ç–µ–ø–µ–Ω—å —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –≤–∏–¥–µ–æ      üü•üü•üü•

–ú–æ–¥—É–ª—å –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ç–µ–ø–µ–Ω–∏ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –≤–∏–¥–µ–æ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∞—Å–ø–µ–∫—Ç–∞–º.
–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ç–µ–∫—É—â–µ–µ –≤–∏–¥–µ–æ —Å —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã–º–∏/—Ç–æ–ø–æ–≤—ã–º–∏ –≤–∏–¥–µ–æ –∏ –≤—ã—á–∏—Å–ª—è–µ—Ç novelty scores.

üîπ A. Semantic / Content Novelty
üîπ B. Visual / Style Novelty
üîπ C. Editing & Pacing Novelty
üîπ D. Audio Novelty
üîπ E. Text / OCR Novelty
üîπ F. Behavioral & Motion Novelty
üîπ G. Multimodal Novelty
üîπ H. Temporal / Trend Novelty

TODO: 
    1. –ù—É–∂–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–∞–ø—Ç—å –∫–æ–¥ –ø–æ–¥ —Ä–∞–±–æ—Ç—É —Å –≤–Ω–µ—à–Ω–∏–º–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏ –∏ —á—Ç–µ–Ω–∏–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å–≤—É—é—â–∏—Ö npz —Ñ–∞–π–ª–æ–≤
    2. –ï—Å–ª–∏ –≤ –º–æ–¥—É–ª–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –º–æ–¥–µ–ª–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –≤ core/model_process, –Ω—É–∂–Ω–æ –∏—Å–∫–ª—é—á–∏—Ç—å –∏—Ö –∏–∑ –º–æ–¥—É–ª—è –∏ –ø–µ—Ä–µ–π—Ç–∏ –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    3. –ù—É–∂–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–∞–ø—Ç—å –º–æ–¥—É–ª—å –ø–æ–¥ —Ä–∞–±–æ—Ç—É —Å BaseModule
    4. –ù—É–∂–Ω–æ –ø—Ä–∏–≤–µ—Å—Ç–∏ –≤—ã—Ö–æ–¥ –∫ –µ–¥–∏–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ npz
"""

from __future__ import annotations

import os
from typing import Dict, List, Optional, Any, Union, Tuple
import numpy as np
from scipy.spatial.distance import cosine
from scipy.stats import pearsonr, entropy, wasserstein_distance
from sklearn.metrics.pairwise import cosine_similarity
import warnings

warnings.filterwarnings("ignore")

from modules.base_module import BaseModule
from utils.frame_manager import FrameManager


class UniquenessModule:
    """
    –ú–æ–¥—É–ª—å –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ç–µ–ø–µ–Ω–∏ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –≤–∏–¥–µ–æ.
    
    –í—ã—á–∏—Å–ª—è–µ—Ç novelty scores –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∞—Å–ø–µ–∫—Ç–∞–º:
    - –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –Ω–æ–≤–∏–∑–Ω–∞ (content novelty)
    - –í–∏–∑—É–∞–ª—å–Ω–∞—è –Ω–æ–≤–∏–∑–Ω–∞ (visual/style novelty)
    - –ù–æ–≤–∏–∑–Ω–∞ –º–æ–Ω—Ç–∞–∂–∞ –∏ —Ä–∏—Ç–º–∞ (editing/pacing novelty)
    - –ê—É–¥–∏–æ –Ω–æ–≤–∏–∑–Ω–∞
    - –¢–µ–∫—Å—Ç–æ–≤–∞—è –Ω–æ–≤–∏–∑–Ω–∞ (OCR/text novelty)
    - –ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∞—è –Ω–æ–≤–∏–∑–Ω–∞ (behavioral/motion novelty)
    - –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –Ω–æ–≤–∏–∑–Ω–∞
    - –í—Ä–µ–º–µ–Ω–Ω–∞—è/—Ç—Ä–µ–Ω–¥–æ–≤–∞—è –Ω–æ–≤–∏–∑–Ω–∞
    """
    
    def __init__(
        self,
        top_n: int = 100,
        novelty_weights: Optional[Dict[str, float]] = None
    ):
        """
        Args:
            top_n: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø –≤–∏–¥–µ–æ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 100)
            novelty_weights: –í–µ—Å–∞ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ overall_novelty_index
        """
        self.top_n = top_n
        
        # –í–µ—Å–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è overall_novelty_index
        self.novelty_weights = novelty_weights or {
            'semantic': 0.20,
            'visual': 0.15,
            'editing': 0.15,
            'audio': 0.10,
            'text': 0.10,
            'behavioral': 0.10,
            'multimodal': 0.15,
            'temporal': 0.05
        }
    
    # ==================== A. Semantic / Content Novelty ====================
    
    def compute_semantic_novelty(
        self,
        video_embedding: np.ndarray,
        reference_embeddings: List[np.ndarray],
        video_topics: Optional[Union[List[str], np.ndarray, Dict[str, float]]] = None,
        reference_topics_list: Optional[List[Union[List[str], np.ndarray, Dict[str, float]]]] = None
    ) -> Dict[str, float]:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –Ω–æ–≤–∏–∑–Ω—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞.
        
        Args:
            video_embedding: Embedding —Ç–µ–∫—É—â–µ–≥–æ –≤–∏–¥–µ–æ (1D array)
            reference_embeddings: –°–ø–∏—Å–æ–∫ embeddings —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –≤–∏–¥–µ–æ
            video_topics: –¢–µ–º—ã —Ç–µ–∫—É—â–µ–≥–æ –≤–∏–¥–µ–æ
            reference_topics_list: –°–ø–∏—Å–æ–∫ —Ç–µ–º —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –≤–∏–¥–µ–æ
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –Ω–æ–≤–∏–∑–Ω—ã
        """
        if len(reference_embeddings) == 0:
            return {
                "semantic_novelty_score": 1.0,
                "semantic_novelty_max": 1.0,
                "semantic_novelty_topk_mean": 1.0,
                "semantic_novelty_topk_median": 1.0,
                "topic_novelty_score": 1.0,
                "concept_diversity_score": 0.0,
                "concept_diversity_entropy": 0.0,
                "concept_diversity_unique_norm": 0.0,
            }

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º embeddings
        video_emb_norm = video_embedding / (np.linalg.norm(video_embedding) + 1e-10)

        similarities: List[float] = []
        for ref_emb in reference_embeddings:
            ref_emb_norm = ref_emb / (np.linalg.norm(ref_emb) + 1e-10)
            sim = float(np.dot(video_emb_norm, ref_emb_norm))
            similarities.append(sim)

        similarities_arr = np.asarray(similarities, dtype=np.float32)
        if similarities_arr.size == 0:
            max_similarity = 0.0
            topk_mean_sim = 0.0
            topk_median_sim = 0.0
        else:
            similarities_sorted = np.sort(similarities_arr)[::-1]
            max_similarity = float(similarities_sorted[0])
            k = min(5, similarities_sorted.size)
            topk = similarities_sorted[:k]
            topk_mean_sim = float(topk.mean())
            topk_median_sim = float(np.median(topk))

        # Semantic novelty family: 1 - similarity
        semantic_novelty_max = 1.0 - max_similarity
        semantic_novelty_topk_mean = 1.0 - topk_mean_sim
        semantic_novelty_topk_median = 1.0 - topk_median_sim

        # Topic novelty: –¥–æ–ª—è –Ω–æ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤
        topic_novelty = 1.0
        if video_topics is not None and reference_topics_list is not None:
            topic_novelty = self._compute_topic_novelty(video_topics, reference_topics_list)

        # Concept diversity: —ç–Ω—Ç—Ä–æ–ø–∏—è + –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π unique count
        diversity_entropy = 0.0
        diversity_unique_norm = 0.0
        if video_topics is not None:
            if isinstance(video_topics, dict):
                probs = np.array(list(video_topics.values()), dtype=np.float32)
                if probs.size > 0:
                    probs = probs / (probs.sum() + 1e-10)
                    diversity_entropy = float(
                        entropy(probs) / (np.log(len(probs) + 1e-10))
                    )
                    diversity_unique_norm = float(
                        len([p for p in probs if p > 0.01]) / np.log(len(probs) + 1.0)
                    )
            elif isinstance(video_topics, (list, np.ndarray)) and len(video_topics) > 0:
                if isinstance(video_topics[0], str):
                    unique_count = len(set(video_topics))
                    total_count = len(video_topics)
                    diversity_unique_norm = float(
                        unique_count / np.log(total_count + 1.0)
                    ) if total_count > 0 else 0.0
                else:
                    arr = np.asarray(video_topics, dtype=np.float32)
                    probs = arr / (arr.sum() + 1e-10)
                    diversity_entropy = float(
                        entropy(probs) / (np.log(len(probs) + 1e-10))
                    )

        # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ concept_diversity_score = entropy-–≤–µ—Ä—Å–∏—è
        concept_diversity_score = diversity_entropy

        return {
            "semantic_novelty_score": float(semantic_novelty_max),
            "semantic_novelty_max": float(semantic_novelty_max),
            "semantic_novelty_topk_mean": float(semantic_novelty_topk_mean),
            "semantic_novelty_topk_median": float(semantic_novelty_topk_median),
            "topic_novelty_score": float(topic_novelty),
            "concept_diversity_score": float(concept_diversity_score),
            "concept_diversity_entropy": float(diversity_entropy),
            "concept_diversity_unique_norm": float(diversity_unique_norm),
        }
    
    def _compute_topic_novelty(
        self,
        video_topics: Union[List[str], np.ndarray, Dict[str, float]],
        reference_topics_list: List[Union[List[str], np.ndarray, Dict[str, float]]]
    ) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –¥–æ–ª—é –Ω–æ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –≤–∏–¥–µ–æ."""
        def to_set(topics):
            if isinstance(topics, dict):
                sorted_topics = sorted(topics.items(), key=lambda x: x[1], reverse=True)
                return set([t[0] for t in sorted_topics[:10]])
            elif isinstance(topics, (list, np.ndarray)):
                if len(topics) > 0 and isinstance(topics[0], str):
                    return set(topics[:20])
                else:
                    return set(np.where(np.array(topics) > 0.1)[0].astype(str))
            return set()
        
        video_topics_set = to_set(video_topics)
        if len(video_topics_set) == 0:
            return 0.0
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–µ–º—ã –∏–∑ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –≤–∏–¥–µ–æ
        all_reference_topics = set()
        for ref_topics in reference_topics_list:
            all_reference_topics.update(to_set(ref_topics))
        
        # –ù–æ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã = —Ç–µ, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö
        new_concepts = video_topics_set - all_reference_topics
        novelty_ratio = len(new_concepts) / len(video_topics_set) if len(video_topics_set) > 0 else 0.0
        
        return float(novelty_ratio)
    
    def _compute_concept_diversity(
        self,
        video_topics: Union[List[str], np.ndarray, Dict[str, float]]
    ) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤."""
        if isinstance(video_topics, dict):
            probs = np.array(list(video_topics.values()))
            probs = probs / (probs.sum() + 1e-10)
            diversity = entropy(probs) / np.log(len(probs) + 1e-10) if len(probs) > 0 else 0.0
            return float(diversity)
        elif isinstance(video_topics, (list, np.ndarray)):
            unique_count = len(set(video_topics)) if isinstance(video_topics[0], str) else len(video_topics)
            total_count = len(video_topics)
            return float(unique_count / total_count) if total_count > 0 else 0.0
        return 0.0
    
    # ==================== B. Visual / Style Novelty ====================
    
    def compute_visual_novelty(
        self,
        video_visual_features: Dict[str, Any],
        reference_visual_features_list: List[Dict[str, Any]]
    ) -> Dict[str, float]:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—É—é –Ω–æ–≤–∏–∑–Ω—É —Å—Ç–∏–ª—è.
        
        Args:
            video_visual_features: –í–∏–∑—É–∞–ª—å–Ω—ã–µ —Ñ–∏—á–∏ —Ç–µ–∫—É—â–µ–≥–æ –≤–∏–¥–µ–æ (—Ü–≤–µ—Ç, —Å–≤–µ—Ç, –∫–æ–º–ø–æ–∑–∏—Ü–∏—è, —Ç–∏–ø—ã –∫–∞–¥—Ä–æ–≤)
            reference_visual_features_list: –°–ø–∏—Å–æ–∫ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ñ–∏—á–µ–π —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –≤–∏–¥–µ–æ
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –≤–∏–∑—É–∞–ª—å–Ω–æ–π –Ω–æ–≤–∏–∑–Ω—ã
        """
        if len(reference_visual_features_list) == 0:
            return {
                'color_palette_novelty': 1.0,
                'lighting_style_novelty': 1.0,
                'shot_type_novelty': 1.0,
                'camera_motion_novelty': 1.0
            }
        
        color_novelties = []
        lighting_novelties = []
        shot_type_novelties = []
        camera_motion_novelties = []
        
        for ref_features in reference_visual_features_list:
            # Color palette novelty
            if 'color_histogram' in video_visual_features and 'color_histogram' in ref_features:
                hist1 = np.array(video_visual_features['color_histogram']).flatten()
                hist2 = np.array(ref_features['color_histogram']).flatten()
                if len(hist1) == len(hist2):
                    hist1_norm = hist1 / (np.linalg.norm(hist1) + 1e-10)
                    hist2_norm = hist2 / (np.linalg.norm(hist2) + 1e-10)
                    sim = np.dot(hist1_norm, hist2_norm)
                    color_novelties.append(1.0 - sim)
            
            # Lighting style novelty
            if 'lighting_features' in video_visual_features and 'lighting_features' in ref_features:
                light1 = np.array(video_visual_features['lighting_features'])
                light2 = np.array(ref_features['lighting_features'])
                if light1.shape == light2.shape:
                    sim = 1.0 - cosine(light1.flatten(), light2.flatten())
                    lighting_novelties.append(max(0.0, 1.0 - sim))
            
            # Shot type novelty
            if 'shot_type_distribution' in video_visual_features and 'shot_type_distribution' in ref_features:
                dist1 = np.array(video_visual_features['shot_type_distribution'])
                dist2 = np.array(ref_features['shot_type_distribution'])
                if len(dist1) == len(dist2):
                    dist1_norm = dist1 / (dist1.sum() + 1e-10)
                    dist2_norm = dist2 / (dist2.sum() + 1e-10)
                    wd = wasserstein_distance(dist1_norm, dist2_norm)
                    max_wd = np.max(dist1_norm) + np.max(dist2_norm)
                    sim = 1.0 - wd / (max_wd + 1e-10)
                    shot_type_novelties.append(max(0.0, min(1.0, 1.0 - sim)))
            
            # Camera motion novelty
            if 'camera_motion_features' in video_visual_features and 'camera_motion_features' in ref_features:
                motion1 = np.array(video_visual_features['camera_motion_features'])
                motion2 = np.array(ref_features['camera_motion_features'])
                if len(motion1) == len(motion2):
                    try:
                        corr, _ = pearsonr(motion1, motion2)
                        motion_novelty = 1.0 - max(0.0, corr) if not np.isnan(corr) else 1.0
                        camera_motion_novelties.append(motion_novelty)
                    except:
                        camera_motion_novelties.append(1.0)
        
        return {
            'color_palette_novelty': float(np.mean(color_novelties)) if color_novelties else 1.0,
            'lighting_style_novelty': float(np.mean(lighting_novelties)) if lighting_novelties else 1.0,
            'shot_type_novelty': float(np.mean(shot_type_novelties)) if shot_type_novelties else 1.0,
            'camera_motion_novelty': float(np.mean(camera_motion_novelties)) if camera_motion_novelties else 1.0
        }
    
    # ==================== C. Editing & Pacing Novelty ====================
    
    def compute_editing_novelty(
        self,
        video_pacing_features: Dict[str, Any],
        reference_pacing_features_list: List[Dict[str, Any]]
    ) -> Dict[str, float]:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –Ω–æ–≤–∏–∑–Ω—É –º–æ–Ω—Ç–∞–∂–∞ –∏ —Ä–∏—Ç–º–∞.
        
        Args:
            video_pacing_features: –§–∏—á–∏ pacing —Ç–µ–∫—É—â–µ–≥–æ –≤–∏–¥–µ–æ (cut rate, shot duration, scene length, pacing curve)
            reference_pacing_features_list: –°–ø–∏—Å–æ–∫ —Ñ–∏—á–µ–π pacing —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –≤–∏–¥–µ–æ
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –Ω–æ–≤–∏–∑–Ω—ã –º–æ–Ω—Ç–∞–∂–∞
        """
        if len(reference_pacing_features_list) == 0:
            return {
                'cut_rate_novelty': 1.0,
                'shot_duration_distribution_novelty': 1.0,
                'scene_length_novelty': 1.0,
                'pacing_pattern_novelty': 1.0
            }
        
        cut_rate_novelties = []
        shot_duration_novelties = []
        scene_length_novelties = []
        pacing_pattern_novelties = []
        
        for ref_features in reference_pacing_features_list:
            # Cut rate novelty
            if 'cut_rate' in video_pacing_features and 'cut_rate' in ref_features:
                cut1 = video_pacing_features['cut_rate']
                cut2 = ref_features['cut_rate']
                max_cut = max(abs(cut1), abs(cut2), 1.0)
                diff = abs(cut1 - cut2) / max_cut
                cut_rate_novelties.append(min(1.0, diff))
            
            # Shot duration distribution novelty
            if 'shot_duration_distribution' in video_pacing_features and 'shot_duration_distribution' in ref_features:
                dist1 = np.array(video_pacing_features['shot_duration_distribution'])
                dist2 = np.array(ref_features['shot_duration_distribution'])
                if len(dist1) == len(dist2):
                    dist1_norm = dist1 / (dist1.sum() + 1e-10)
                    dist2_norm = dist2 / (dist2.sum() + 1e-10)
                    wd = wasserstein_distance(dist1_norm, dist2_norm)
                    max_wd = np.max(dist1_norm) + np.max(dist2_norm)
                    sim = 1.0 - wd / (max_wd + 1e-10)
                    shot_duration_novelties.append(max(0.0, min(1.0, 1.0 - sim)))
            
            # Scene length novelty
            if 'scene_lengths' in video_pacing_features and 'scene_lengths' in ref_features:
                len1 = np.array(video_pacing_features['scene_lengths'])
                len2 = np.array(ref_features['scene_lengths'])
                if len(len1) > 0 and len(len2) > 0:
                    mean1, std1 = np.mean(len1), np.std(len1)
                    mean2, std2 = np.mean(len2), np.std(len2)
                    max_mean = max(abs(mean1), abs(mean2), 1.0)
                    max_std = max(abs(std1), abs(std2), 1.0)
                    mean_diff = abs(mean1 - mean2) / max_mean
                    std_diff = abs(std1 - std2) / max_std
                    scene_novelty = (mean_diff + std_diff) / 2.0
                    scene_length_novelties.append(min(1.0, scene_novelty))
            
            # Pacing pattern novelty
            if 'pacing_curve' in video_pacing_features and 'pacing_curve' in ref_features:
                curve1 = np.array(video_pacing_features['pacing_curve'])
                curve2 = np.array(ref_features['pacing_curve'])
                if len(curve1) == len(curve2):
                    try:
                        corr, _ = pearsonr(curve1, curve2)
                        pacing_novelty = 1.0 - max(0.0, corr) if not np.isnan(corr) else 1.0
                        pacing_pattern_novelties.append(pacing_novelty)
                    except:
                        pacing_pattern_novelties.append(1.0)
        
        return {
            'cut_rate_novelty': float(np.mean(cut_rate_novelties)) if cut_rate_novelties else 1.0,
            'shot_duration_distribution_novelty': float(np.mean(shot_duration_novelties)) if shot_duration_novelties else 1.0,
            'scene_length_novelty': float(np.mean(scene_length_novelties)) if scene_length_novelties else 1.0,
            'pacing_pattern_novelty': float(np.mean(pacing_pattern_novelties)) if pacing_pattern_novelties else 1.0
        }
    
    # ==================== D. Audio Novelty ====================
    
    def compute_audio_novelty(
        self,
        video_audio_features: Dict[str, Any],
        reference_audio_features_list: List[Dict[str, Any]]
    ) -> Dict[str, float]:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –∞—É–¥–∏–æ –Ω–æ–≤–∏–∑–Ω—É.
        
        Args:
            video_audio_features: –ê—É–¥–∏–æ —Ñ–∏—á–∏ —Ç–µ–∫—É—â–µ–≥–æ –≤–∏–¥–µ–æ (embeddings, tempo, energy, voice style)
            reference_audio_features_list: –°–ø–∏—Å–æ–∫ –∞—É–¥–∏–æ —Ñ–∏—á–µ–π —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –≤–∏–¥–µ–æ
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∞—É–¥–∏–æ –Ω–æ–≤–∏–∑–Ω—ã
        """
        if len(reference_audio_features_list) == 0:
            return {
                'music_track_novelty': 1.0,
                'voice_style_novelty': 1.0,
                'sound_effects_novelty': 1.0,
                'audio_energy_pattern_novelty': 1.0
        }
        
        music_novelties = []
        voice_novelties = []
        sound_effects_novelties = []
        energy_pattern_novelties = []
        
        for ref_features in reference_audio_features_list:
            # Music track novelty (BPM, style)
            if 'audio_embedding' in video_audio_features and 'audio_embedding' in ref_features:
                emb1 = np.array(video_audio_features['audio_embedding'])
                emb2 = np.array(ref_features['audio_embedding'])
                if emb1.shape == emb2.shape:
                    emb1_norm = emb1 / (np.linalg.norm(emb1) + 1e-10)
                    emb2_norm = emb2 / (np.linalg.norm(emb2) + 1e-10)
                    sim = np.dot(emb1_norm, emb2_norm)
                    music_novelties.append(1.0 - sim)
            
            # Voice style novelty
            if 'voice_embedding' in video_audio_features and 'voice_embedding' in ref_features:
                voice1 = np.array(video_audio_features['voice_embedding'])
                voice2 = np.array(ref_features['voice_embedding'])
                if voice1.shape == voice2.shape:
                    voice1_norm = voice1 / (np.linalg.norm(voice1) + 1e-10)
                    voice2_norm = voice2 / (np.linalg.norm(voice2) + 1e-10)
                    sim = np.dot(voice1_norm, voice2_norm)
                    voice_novelties.append(1.0 - sim)
            elif 'tempo' in video_audio_features and 'tempo' in ref_features:
                # Fallback: use tempo difference
                tempo1 = video_audio_features['tempo']
                tempo2 = ref_features['tempo']
                max_tempo = max(abs(tempo1), abs(tempo2), 1.0)
                tempo_diff = abs(tempo1 - tempo2) / max_tempo
                voice_novelties.append(min(1.0, tempo_diff))
            
            # Sound effects novelty
            if 'sound_effects_features' in video_audio_features and 'sound_effects_features' in ref_features:
                sfx1 = np.array(video_audio_features['sound_effects_features'])
                sfx2 = np.array(ref_features['sound_effects_features'])
                if sfx1.shape == sfx2.shape:
                    sfx1_norm = sfx1 / (np.linalg.norm(sfx1) + 1e-10)
                    sfx2_norm = sfx2 / (np.linalg.norm(sfx2) + 1e-10)
                    sim = np.dot(sfx1_norm, sfx2_norm)
                    sound_effects_novelties.append(1.0 - sim)
            
            # Audio energy pattern novelty
            if 'energy_pattern' in video_audio_features and 'energy_pattern' in ref_features:
                energy1 = np.array(video_audio_features['energy_pattern'])
                energy2 = np.array(ref_features['energy_pattern'])
                if len(energy1) == len(energy2):
                    try:
                        corr, _ = pearsonr(energy1, energy2)
                        energy_novelty = 1.0 - max(0.0, corr) if not np.isnan(corr) else 1.0
                        energy_pattern_novelties.append(energy_novelty)
                    except:
                        energy_pattern_novelties.append(1.0)
        
        return {
            'music_track_novelty': float(np.mean(music_novelties)) if music_novelties else 1.0,
            'voice_style_novelty': float(np.mean(voice_novelties)) if voice_novelties else 1.0,
            'sound_effects_novelty': float(np.mean(sound_effects_novelties)) if sound_effects_novelties else 1.0,
            'audio_energy_pattern_novelty': float(np.mean(energy_pattern_novelties)) if energy_pattern_novelties else 1.0
        }
    
    # ==================== E. Text / OCR Novelty ====================
    
    def compute_text_novelty(
        self,
        video_text_features: Dict[str, Any],
        reference_text_features_list: List[Dict[str, Any]]
    ) -> Dict[str, float]:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—É—é –Ω–æ–≤–∏–∑–Ω—É (OCR, –∑–∞–≥–æ–ª–æ–≤–∫–∏, –º–µ–º—ã).
        
        Args:
            video_text_features: –¢–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∏—á–∏ —Ç–µ–∫—É—â–µ–≥–æ –≤–∏–¥–µ–æ (OCR embeddings, layout, style)
            reference_text_features_list: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∏—á–µ–π —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –≤–∏–¥–µ–æ
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–π –Ω–æ–≤–∏–∑–Ω—ã
        """
        if len(reference_text_features_list) == 0:
            return {
                'ocr_text_novelty': 1.0,
                'text_layout_novelty': 1.0,
                'text_style_novelty': 1.0
            }
        
        ocr_novelties = []
        layout_novelties = []
        style_novelties = []
        
        for ref_features in reference_text_features_list:
            # OCR text novelty (–Ω–æ–≤—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞/—Ñ—Ä–∞–∑—ã)
            if 'ocr_embedding' in video_text_features and 'ocr_embedding' in ref_features:
                emb1 = np.array(video_text_features['ocr_embedding'])
                emb2 = np.array(ref_features['ocr_embedding'])
                if emb1.shape == emb2.shape:
                    emb1_norm = emb1 / (np.linalg.norm(emb1) + 1e-10)
                    emb2_norm = emb2 / (np.linalg.norm(emb2) + 1e-10)
                    sim = np.dot(emb1_norm, emb2_norm)
                    ocr_novelties.append(1.0 - sim)
            
            # Text layout novelty (–Ω–µ–æ–±—ã—á–Ω–æ–µ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞)
            if 'text_layout' in video_text_features and 'text_layout' in ref_features:
                layout1 = np.array(video_text_features['text_layout'])
                layout2 = np.array(ref_features['text_layout'])
                if len(layout1) == len(layout2):
                    layout_sim = 1.0 - cosine(layout1.flatten(), layout2.flatten())
                    layout_novelties.append(max(0.0, 1.0 - layout_sim))
            
            # Text style novelty (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —à—Ä–∏—Ñ—Ç—ã, —Ü–≤–µ—Ç–∞, —ç—Ñ—Ñ–µ–∫—Ç—ã)
            if 'text_style_features' in video_text_features and 'text_style_features' in ref_features:
                style1 = np.array(video_text_features['text_style_features'])
                style2 = np.array(ref_features['text_style_features'])
                if style1.shape == style2.shape:
                    style1_norm = style1 / (np.linalg.norm(style1) + 1e-10)
                    style2_norm = style2 / (np.linalg.norm(style2) + 1e-10)
                    sim = np.dot(style1_norm, style2_norm)
                    style_novelties.append(1.0 - sim)
        
        return {
            'ocr_text_novelty': float(np.mean(ocr_novelties)) if ocr_novelties else 1.0,
            'text_layout_novelty': float(np.mean(layout_novelties)) if layout_novelties else 1.0,
            'text_style_novelty': float(np.mean(style_novelties)) if style_novelties else 1.0
        }
    
    # ==================== F. Behavioral & Motion Novelty ====================
    
    def compute_behavioral_novelty(
        self,
        video_behavior_features: Dict[str, Any],
        reference_behavior_features_list: List[Dict[str, Any]]
    ) -> Dict[str, float]:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫—É—é –Ω–æ–≤–∏–∑–Ω—É (–¥–≤–∏–∂–µ–Ω–∏—è –ª—é–¥–µ–π, –æ–±—ä–µ–∫—Ç–æ–≤).
        
        Args:
            video_behavior_features: –§–∏—á–∏ –ø–æ–≤–µ–¥–µ–Ω–∏—è —Ç–µ–∫—É—â–µ–≥–æ –≤–∏–¥–µ–æ (pose motion, object interaction, actions)
            reference_behavior_features_list: –°–ø–∏—Å–æ–∫ —Ñ–∏—á–µ–π –ø–æ–≤–µ–¥–µ–Ω–∏—è —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –≤–∏–¥–µ–æ
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–æ–π –Ω–æ–≤–∏–∑–Ω—ã
        """
        if len(reference_behavior_features_list) == 0:
            return {
                'pose_motion_novelty': 1.0,
                'object_interaction_novelty': 1.0,
                'action_sequence_novelty': 1.0
            }
        
        pose_novelties = []
        interaction_novelties = []
        action_novelties = []
        
        for ref_features in reference_behavior_features_list:
            # Pose motion novelty (–Ω–µ–æ–±—ã—á–Ω—ã–µ –¥–≤–∏–∂–µ–Ω–∏—è/—Ä–µ–¥–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã)
            if 'pose_motion' in video_behavior_features and 'pose_motion' in ref_features:
                pose1 = np.array(video_behavior_features['pose_motion'])
                pose2 = np.array(ref_features['pose_motion'])
                if pose1.shape == pose2.shape:
                    pose1_norm = pose1 / (np.linalg.norm(pose1) + 1e-10)
                    pose2_norm = pose2 / (np.linalg.norm(pose2) + 1e-10)
                    sim = np.dot(pose1_norm.flatten(), pose2_norm.flatten())
                    pose_novelties.append(1.0 - sim)
            
            # Object interaction novelty
            if 'object_interaction' in video_behavior_features and 'object_interaction' in ref_features:
                int1 = np.array(video_behavior_features['object_interaction'])
                int2 = np.array(ref_features['object_interaction'])
                if int1.shape == int2.shape:
                    int1_norm = int1 / (np.linalg.norm(int1) + 1e-10)
                    int2_norm = int2 / (np.linalg.norm(int2) + 1e-10)
                    sim = np.dot(int1_norm.flatten(), int2_norm.flatten())
                    interaction_novelties.append(1.0 - sim)
            
            # Action sequence novelty
            if 'action_sequence' in video_behavior_features and 'action_sequence' in ref_features:
                act1 = np.array(video_behavior_features['action_sequence'])
                act2 = np.array(ref_features['action_sequence'])
                if len(act1) == len(act2):
                    try:
                        corr, _ = pearsonr(act1, act2)
                        action_novelty = 1.0 - max(0.0, corr) if not np.isnan(corr) else 1.0
                        action_novelties.append(action_novelty)
                    except:
                        action_novelties.append(1.0)
        
        return {
            'pose_motion_novelty': float(np.mean(pose_novelties)) if pose_novelties else 1.0,
            'object_interaction_novelty': float(np.mean(interaction_novelties)) if interaction_novelties else 1.0,
            'action_sequence_novelty': float(np.mean(action_novelties)) if action_novelties else 1.0
        }
    
    # ==================== G. Multimodal Novelty ====================
    
    def compute_multimodal_novelty(
        self,
        all_novelty_metrics: Dict[str, float],
        video_events: Optional[List[Dict[str, Any]]] = None,
        reference_events_list: Optional[List[List[Dict[str, Any]]]] = None
    ) -> Dict[str, float]:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –Ω–æ–≤–∏–∑–Ω—É.
        
        Args:
            all_novelty_metrics: –í—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –Ω–æ–≤–∏–∑–Ω—ã –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–π A-F
            video_events: –°–æ–±—ã—Ç–∏—è —Ç–µ–∫—É—â–µ–≥–æ –≤–∏–¥–µ–æ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            reference_events_list: –°–ø–∏—Å–æ–∫ —Å–æ–±—ã—Ç–∏–π —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –≤–∏–¥–µ–æ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –Ω–æ–≤–∏–∑–Ω—ã
        """
        # Multimodal novelty score = —Å—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω–∞—è –Ω–æ–≤–∏–∑–Ω–∞ –ø–æ –≤—Å–µ–º –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º
        weights = self.novelty_weights
        
        semantic_novelty = all_novelty_metrics.get('semantic_novelty_score', 0.0)
        visual_novelty = np.mean([
            all_novelty_metrics.get('color_palette_novelty', 0.0),
            all_novelty_metrics.get('lighting_style_novelty', 0.0),
            all_novelty_metrics.get('shot_type_novelty', 0.0)
        ])
        editing_novelty = np.mean([
            all_novelty_metrics.get('cut_rate_novelty', 0.0),
            all_novelty_metrics.get('pacing_pattern_novelty', 0.0)
        ])
        audio_novelty = all_novelty_metrics.get('music_track_novelty', 0.0)
        text_novelty = all_novelty_metrics.get('ocr_text_novelty', 0.0)
        behavioral_novelty = all_novelty_metrics.get('pose_motion_novelty', 0.0)
        
        multimodal_novelty = (
            weights['semantic'] * semantic_novelty +
            weights['visual'] * visual_novelty +
            weights['editing'] * editing_novelty +
            weights['audio'] * audio_novelty +
            weights['text'] * text_novelty +
            weights['behavioral'] * behavioral_novelty
        )
        
        # Novel event alignment score (–Ω–æ–≤—ã–µ —Å–æ–±—ã—Ç–∏—è, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ —Ç–æ–ø –≤–∏–¥–µ–æ)
        novel_event_score = 1.0
        if video_events is not None and reference_events_list is not None:
            novel_event_score = self._compute_novel_event_score(video_events, reference_events_list)
        
        # Overall novelty index
        overall_novelty = (
            weights['semantic'] * semantic_novelty +
            weights['visual'] * visual_novelty +
            weights['editing'] * editing_novelty +
            weights['audio'] * audio_novelty +
            weights['text'] * text_novelty +
            weights['behavioral'] * behavioral_novelty +
            weights['multimodal'] * multimodal_novelty +
            weights['temporal'] * all_novelty_metrics.get('trend_alignment_score', 0.0)
        )
        
        return {
            'multimodal_novelty_score': float(multimodal_novelty),
            'novel_event_alignment_score': float(novel_event_score),
            'overall_novelty_index': float(overall_novelty)
        }
    
    def _compute_novel_event_score(
        self,
        video_events: List[Dict[str, Any]],
        reference_events_list: List[List[Dict[str, Any]]]
    ) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –¥–æ–ª—é –Ω–æ–≤—ã—Ö —Å–æ–±—ã—Ç–∏–π, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –≤–∏–¥–µ–æ."""
        if len(video_events) == 0:
            return 0.0
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–∏–ø—ã —Å–æ–±—ã—Ç–∏–π –∏–∑ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –≤–∏–¥–µ–æ
        all_reference_event_types = set()
        for ref_events in reference_events_list:
            for event in ref_events:
                event_type = event.get('type', event.get('event_type', ''))
                if event_type:
                    all_reference_event_types.add(event_type)
        
        # –ù–æ–≤—ã–µ —Å–æ–±—ã—Ç–∏—è = —Ç–µ, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö
        video_event_types = set()
        for event in video_events:
            event_type = event.get('type', event.get('event_type', ''))
            if event_type:
                video_event_types.add(event_type)
        
        new_event_types = video_event_types - all_reference_event_types
        novelty_ratio = len(new_event_types) / len(video_event_types) if len(video_event_types) > 0 else 0.0
        
        return float(novelty_ratio)
    
    # ==================== H. Temporal / Trend Novelty ====================
    
    def compute_temporal_novelty(
        self,
        video_metadata: Optional[Dict[str, Any]] = None,
        reference_videos_metadata: Optional[List[Dict[str, Any]]] = None,
        similarity_scores: Optional[Dict[str, float]] = None
    ) -> Dict[str, float]:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—É—é/—Ç—Ä–µ–Ω–¥–æ–≤—É—é –Ω–æ–≤–∏–∑–Ω—É.
        
        Args:
            video_metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ç–µ–∫—É—â–µ–≥–æ –≤–∏–¥–µ–æ (–¥–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è, –∫–∞—Ç–µ–≥–æ—Ä–∏—è –∏ —Ç.–¥.)
            reference_videos_metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –≤–∏–¥–µ–æ
            similarity_scores: –ü—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–µ similarity scores (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –Ω–æ–≤–∏–∑–Ω—ã
        """
        # Trend alignment score = –Ω–∞—Å–∫–æ–ª—å–∫–æ –≤–∏–¥–µ–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç–µ–∫—É—â–µ–º—É —Ç—Ä–µ–Ω–¥—É (–Ω–∏–∑–∫–∏–π ‚Üí —É–Ω–∏–∫–∞–ª—å–Ω–æ)
        trend_alignment = 0.5  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        if similarity_scores is not None:
            # –ï—Å–ª–∏ –µ—Å—Ç—å similarity scores, trend alignment = —Å—Ä–µ–¥–Ω—è—è —Å—Ö–æ–∂–µ—Å—Ç—å —Å —Ç–æ–ø –≤–∏–¥–µ–æ
            trend_alignment = similarity_scores.get('overall_similarity_score', 0.5)
        elif reference_videos_metadata is not None and len(reference_videos_metadata) > 0:
            # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è: –µ—Å–ª–∏ –≤–∏–¥–µ–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ ‚Üí –≤—ã—Å–æ–∫–∏–π trend alignment
            trend_alignment = 0.5  # –ú–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å, –∏—Å–ø–æ–ª—å–∑—É—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏
        
        # Historical similarity score = —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ø—Ä–æ—à–ª—ã–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
        historical_similarity = 0.5  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        if video_metadata is not None and reference_videos_metadata is not None:
            # –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–∞—Ç—É —Å–æ–∑–¥–∞–Ω–∏—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
            # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è
            historical_similarity = 0.5
        
        # Early adopter score = –Ω–∞—Å–∫–æ–ª—å–∫–æ –≤–∏–¥–µ–æ –Ω–æ–≤–∞—Ç–æ—Ä—Å–∫–æ–µ –≤ —Å–≤–æ–µ–π –Ω–∏—à–µ
        early_adopter = 1.0 - trend_alignment  # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è
        
        return {
            'trend_alignment_score': float(trend_alignment),
            'historical_similarity_score': float(historical_similarity),
            'early_adopter_score': float(early_adopter)
        }
    
    # ==================== Main Method ====================
    
    def extract_all(
        self,
        video_embedding: np.ndarray,
        reference_embeddings: List[np.ndarray],
        video_topics: Optional[Union[List[str], np.ndarray, Dict[str, float]]] = None,
        reference_topics_list: Optional[List[Union[List[str], np.ndarray, Dict[str, float]]]] = None,
        video_visual_features: Optional[Dict[str, Any]] = None,
        reference_visual_features_list: Optional[List[Dict[str, Any]]] = None,
        video_pacing_features: Optional[Dict[str, Any]] = None,
        reference_pacing_features_list: Optional[List[Dict[str, Any]]] = None,
        video_audio_features: Optional[Dict[str, Any]] = None,
        reference_audio_features_list: Optional[List[Dict[str, Any]]] = None,
        video_text_features: Optional[Dict[str, Any]] = None,
        reference_text_features_list: Optional[List[Dict[str, Any]]] = None,
        video_behavior_features: Optional[Dict[str, Any]] = None,
        reference_behavior_features_list: Optional[List[Dict[str, Any]]] = None,
        video_events: Optional[List[Dict[str, Any]]] = None,
        reference_events_list: Optional[List[List[Dict[str, Any]]]] = None,
        video_metadata: Optional[Dict[str, Any]] = None,
        reference_videos_metadata: Optional[List[Dict[str, Any]]] = None,
        similarity_scores: Optional[Dict[str, float]] = None
    ) -> Dict[str, Any]:
        """
        –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏.
        
        Args:
            video_embedding: Embedding —Ç–µ–∫—É—â–µ–≥–æ –≤–∏–¥–µ–æ
            reference_embeddings: –°–ø–∏—Å–æ–∫ embeddings —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –≤–∏–¥–µ–æ
            video_topics: –¢–µ–º—ã —Ç–µ–∫—É—â–µ–≥–æ –≤–∏–¥–µ–æ
            reference_topics_list: –°–ø–∏—Å–æ–∫ —Ç–µ–º —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –≤–∏–¥–µ–æ
            video_visual_features: –í–∏–∑—É–∞–ª—å–Ω—ã–µ —Ñ–∏—á–∏ —Ç–µ–∫—É—â–µ–≥–æ –≤–∏–¥–µ–æ
            reference_visual_features_list: –°–ø–∏—Å–æ–∫ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —Ñ–∏—á–µ–π —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –≤–∏–¥–µ–æ
            video_pacing_features: –§–∏—á–∏ pacing —Ç–µ–∫—É—â–µ–≥–æ –≤–∏–¥–µ–æ
            reference_pacing_features_list: –°–ø–∏—Å–æ–∫ —Ñ–∏—á–µ–π pacing —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –≤–∏–¥–µ–æ
            video_audio_features: –ê—É–¥–∏–æ —Ñ–∏—á–∏ —Ç–µ–∫—É—â–µ–≥–æ –≤–∏–¥–µ–æ
            reference_audio_features_list: –°–ø–∏—Å–æ–∫ –∞—É–¥–∏–æ —Ñ–∏—á–µ–π —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –≤–∏–¥–µ–æ
            video_text_features: –¢–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∏—á–∏ —Ç–µ–∫—É—â–µ–≥–æ –≤–∏–¥–µ–æ
            reference_text_features_list: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∏—á–µ–π —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –≤–∏–¥–µ–æ
            video_behavior_features: –§–∏—á–∏ –ø–æ–≤–µ–¥–µ–Ω–∏—è —Ç–µ–∫—É—â–µ–≥–æ –≤–∏–¥–µ–æ
            reference_behavior_features_list: –°–ø–∏—Å–æ–∫ —Ñ–∏—á–µ–π –ø–æ–≤–µ–¥–µ–Ω–∏—è —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –≤–∏–¥–µ–æ
            video_events: –°–æ–±—ã—Ç–∏—è —Ç–µ–∫—É—â–µ–≥–æ –≤–∏–¥–µ–æ
            reference_events_list: –°–ø–∏—Å–æ–∫ —Å–æ–±—ã—Ç–∏–π —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –≤–∏–¥–µ–æ
            video_metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ç–µ–∫—É—â–µ–≥–æ –≤–∏–¥–µ–æ
            reference_videos_metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã—Ö –≤–∏–¥–µ–æ
            similarity_scores: –ü—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–µ similarity scores (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ –≤—Å–µ–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
        """
        features = {}
        
        # A. Semantic / Content Novelty
        semantic_novelty = self.compute_semantic_novelty(
            video_embedding, reference_embeddings,
            video_topics, reference_topics_list
        )
        features.update(semantic_novelty)
        
        # B. Visual / Style Novelty
        if video_visual_features is not None and reference_visual_features_list is not None:
            visual_novelty = self.compute_visual_novelty(
                video_visual_features, reference_visual_features_list
            )
            features.update(visual_novelty)
        else:
            features.update({
                'color_palette_novelty': 0.0,
                'lighting_style_novelty': 0.0,
                'shot_type_novelty': 0.0,
                'camera_motion_novelty': 0.0
            })
        
        # C. Editing & Pacing Novelty
        if video_pacing_features is not None and reference_pacing_features_list is not None:
            editing_novelty = self.compute_editing_novelty(
                video_pacing_features, reference_pacing_features_list
            )
            features.update(editing_novelty)
        else:
            features.update({
                'cut_rate_novelty': 0.0,
                'shot_duration_distribution_novelty': 0.0,
                'scene_length_novelty': 0.0,
                'pacing_pattern_novelty': 0.0
            })
        
        # D. Audio Novelty
        if video_audio_features is not None and reference_audio_features_list is not None:
            audio_novelty = self.compute_audio_novelty(
                video_audio_features, reference_audio_features_list
            )
            features.update(audio_novelty)
        else:
            features.update({
                'music_track_novelty': 0.0,
                'voice_style_novelty': 0.0,
                'sound_effects_novelty': 0.0,
                'audio_energy_pattern_novelty': 0.0
            })
        
        # E. Text / OCR Novelty
        if video_text_features is not None and reference_text_features_list is not None:
            text_novelty = self.compute_text_novelty(
                video_text_features, reference_text_features_list
            )
            features.update(text_novelty)
        else:
            features.update({
                'ocr_text_novelty': 0.0,
                'text_layout_novelty': 0.0,
                'text_style_novelty': 0.0
            })
        
        # F. Behavioral & Motion Novelty
        if video_behavior_features is not None and reference_behavior_features_list is not None:
            behavioral_novelty = self.compute_behavioral_novelty(
                video_behavior_features, reference_behavior_features_list
            )
            features.update(behavioral_novelty)
        else:
            features.update({
                'pose_motion_novelty': 0.0,
                'object_interaction_novelty': 0.0,
                'action_sequence_novelty': 0.0
            })
        
        # G. Multimodal Novelty
        multimodal_novelty = self.compute_multimodal_novelty(
            features, video_events, reference_events_list
        )
        features.update(multimodal_novelty)
        
        # H. Temporal / Trend Novelty
        temporal_novelty = self.compute_temporal_novelty(
            video_metadata, reference_videos_metadata, similarity_scores
        )
        features.update(temporal_novelty)
        
        return {
            'features': features,
            'all_metrics': features  # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        }


def _normalize_rows(x: np.ndarray) -> np.ndarray:
    norms = np.linalg.norm(x, axis=1, keepdims=True) + 1e-9
    return x / norms


def _load_core_clip_embeddings_aligned(rs_path: str, want_frame_indices: np.ndarray) -> np.ndarray:
    """
    Load core_clip embeddings and align to requested frame_indices (union-domain).
    Requires full coverage (no gaps). No fallback.
    """
    core_path = os.path.join(rs_path, "core_clip", "embeddings.npz")
    if not os.path.isfile(core_path):
        raise FileNotFoundError(f"uniqueness | missing core_clip embeddings: {core_path}")
    data = np.load(core_path, allow_pickle=True)
    core_idx = data.get("frame_indices")
    core_emb = data.get("frame_embeddings")
    if core_idx is None or core_emb is None:
        raise RuntimeError("uniqueness | core_clip embeddings.npz missing keys frame_indices/frame_embeddings")
    core_idx = np.asarray(core_idx, dtype=np.int32)
    core_emb = np.asarray(core_emb, dtype=np.float32)

    mapping = {int(fi): i for i, fi in enumerate(core_idx.tolist())}
    pos = [mapping.get(int(fi), -1) for fi in want_frame_indices.tolist()]
    if any(p < 0 for p in pos):
        raise RuntimeError(
            "uniqueness | core_clip does not cover requested frame_indices. "
            "Segmenter must provide consistent indices across core_clip and this module."
        )
    return core_emb[np.asarray(pos, dtype=np.int64)]


class UniquenessBaselineModule(BaseModule):
    """
    Baseline version of uniqueness:
    - No external reference videos.
    - Computes intra-video repetition/diversity proxies using `core_clip` embeddings.
    """

    @property
    def module_name(self) -> str:
        return "uniqueness"

    def __init__(self, rs_path: Optional[str] = None, repeat_threshold: float = 0.97, **kwargs: Any):
        super().__init__(rs_path=rs_path, logger_name=self.module_name, **kwargs)
        self._repeat_threshold = float(repeat_threshold)

    def required_dependencies(self) -> List[str]:
        return ["core_clip"]

    def process(self, frame_manager: FrameManager, frame_indices: List[int], config: Dict[str, Any]) -> Dict[str, Any]:
        self.initialize()

        if not frame_indices:
            raise ValueError("uniqueness | frame_indices is empty")
        if self.rs_path is None:
            raise ValueError("uniqueness | rs_path is required")

        repeat_thr = float(config.get("repeat_threshold", self._repeat_threshold))
        fi = np.asarray([int(i) for i in frame_indices], dtype=np.int32)

        emb = _load_core_clip_embeddings_aligned(self.rs_path, fi)
        if emb.ndim != 2 or emb.shape[0] != fi.shape[0]:
            raise RuntimeError("uniqueness | invalid embeddings shape after alignment")

        emb_n = _normalize_rows(emb)
        n = int(emb_n.shape[0])

        # Pairwise similarity matrix (N x N), manageable for N~<=800
        sim = emb_n @ emb_n.T
        np.fill_diagonal(sim, -np.inf)

        max_sim_other = np.max(sim, axis=1).astype(np.float32) if n > 0 else np.asarray([], dtype=np.float32)
        repetition_ratio = float(np.mean(max_sim_other >= repeat_thr)) if n > 0 else float("nan")

        if n >= 2:
            iu = np.triu_indices(n, k=1)
            sim_ut = (emb_n @ emb_n.T)[iu].astype(np.float32)
            pairwise_sim_mean = float(np.mean(sim_ut))
            pairwise_sim_p95 = float(np.percentile(sim_ut, 95))
        else:
            sim_ut = np.asarray([], dtype=np.float32)
            pairwise_sim_mean = float("nan")
            pairwise_sim_p95 = float("nan")

        if n >= 2:
            cos_sim_next = np.sum(emb_n[1:] * emb_n[:-1], axis=1).astype(np.float32)
            cos_dist_next = (1.0 - cos_sim_next).astype(np.float32)
            temporal_change_mean = float(np.mean(cos_dist_next))
            temporal_change_std = float(np.std(cos_dist_next))
        else:
            cos_dist_next = np.asarray([], dtype=np.float32)
            temporal_change_mean = float("nan")
            temporal_change_std = float("nan")

        diversity_score = float(
            np.clip(
                1.0 - (pairwise_sim_mean if not np.isnan(pairwise_sim_mean) else 0.0),
                0.0,
                1.0,
            )
        )

        features = {
            "repeat_threshold": float(repeat_thr),
            "repetition_ratio": float(repetition_ratio),
            "pairwise_sim_mean": float(pairwise_sim_mean),
            "pairwise_sim_p95": float(pairwise_sim_p95),
            "temporal_change_mean": float(temporal_change_mean),
            "temporal_change_std": float(temporal_change_std),
            "diversity_score": float(diversity_score),
            "n_frames": int(n),
        }

        return {
            "frame_indices": fi,
            "max_sim_to_other": max_sim_other,
            "cos_dist_next": cos_dist_next,
            "features": np.asarray(features, dtype=object),
        }


# ==================== Example Usage ====================

if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    import numpy as np
    
    # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä
    uniqueness = UniquenessModule(top_n=100)
    
    # –ü—Ä–∏–º–µ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç–µ–∫—É—â–µ–≥–æ –≤–∏–¥–µ–æ
    video_embedding = np.random.randn(512)
    video_topics = ["cooking", "tutorial", "food", "unique_recipe"]
    
    # –†–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—ã–µ –≤–∏–¥–µ–æ
    reference_embeddings = [np.random.randn(512) for _ in range(10)]
    reference_topics_list = [
        ["cooking", "recipe"],
        ["gaming", "tutorial"],
        ["cooking", "food", "tutorial"],
        ["travel", "vlog"],
        ["cooking", "diy"]
    ]
    
    # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
    result = uniqueness.extract_all(
        video_embedding=video_embedding,
        reference_embeddings=reference_embeddings,
        video_topics=video_topics,
        reference_topics_list=reference_topics_list
    )
    
    print("Uniqueness metrics:")
    for key, value in result['features'].items():
        print(f"  {key}: {value:.4f}")
