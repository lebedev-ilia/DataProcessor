## Обзор модулей VisualProcessor

### object_detection
Модуль детектирует объекты на видео: по умолчанию через YOLO11 (COCO‑классы), опционально через open‑vocabulary детекторы OWL‑ViT / OWLv2 по текстовым промптам. Он формирует per‑frame списки объектов с bbox, классами и вероятностями. Модуль служит унифицированным источником объектных фич для композиции, семантики, новизны и других аналитических блоков.

### scene_classification
Модуль определяет тип сцены/окружения (улица, кухня, офис, природа и т.п.) по кадрам, используя модели Places365 и/или современные CNN/ViT из `timm`. Он строит вероятностное распределение по классам сцен и агрегирует их по видео, давая глобальный «фон» и контекст. Эти фичи используются для high‑level семантики, новизны и моделей рекомендаций.

### face_detection
Модуль находит лица в кадрах с помощью InsightFace `FaceAnalysis`. Для каждого кадра он возвращает список bbox и базовых атрибутов (количество лиц, позиции, базовые оценки качества/доверия). Результат служит входом для модулей `emotion_face`, `detalize_face_modules`, `behavioral` и других блоков, где требуется уже детектированное лицо.

### detalize_face_modules
Это «face‑detail» пайплайн, который берёт найденные лица и извлекает детальные фичи по каждому: геометрию, освещение, кожу, структуру, 3D‑параметры, качество, lip‑reading, профессиональные метрики и др. Архитектура реализована как набор подмодулей (`_modules/*`), каждый отвечает за свой блок признаков. Эти фичи используются downstream‑модулями (shot_quality, behavioral, uniqueness) как богатое представление лица.

### emotion_face
Модуль оценивает макро‑эмоции по лицам в кадрах, используя EmoNet и вспомогательную обработку. Он выдаёт вероятности эмоций (happy, sad, anger и др.), интенсивности и устойчивые тренды во времени, а также агрегаты по видео. Эти признаки описывают эмоциональный тон и помогают моделям удержания/рекомендаций и модулю uniqueness.

### behavioral
Модуль анализирует поведение людей в кадре: позы, жесты, динамику тела и базовые поведенческие паттерны. Он опирается на Mediapipe (pose, hands, face mesh) и собственный анализатор, чтобы вычислять фичи про активность, уверенность, открытость, внимание и взаимодействие с камерой/объектами. Результат — агрегаты и временные кривые, описывающие невербальную коммуникацию и динамику поведения.

### optical_flow
Модуль вычисляет оптический поток (RAFT или Farneback) между кадрами и превращает его в метрики движения. Он строит кривые интенсивности движения, распределение направлений и агрегаты (средняя скорость, всплески, хаотичность/стабильность). Эти сигналы используются в `video_pacing`, `story_structure`, `cut_detection`, `text_scoring` и других модулях как универсальный визуальный motion‑сигнал.

### action_recognition
Модуль распознаёт действия/активности на видео (например, «бежит», «танцует», «готовит») с помощью видеомоделей (VideoMAE и др.). Он обрабатывает фрагменты видео и выдаёт per‑clip или per‑video вероятности action‑классов. Эти признаки служат high‑level семантическим слоем для сюжетного анализа, поведенческих моделей и новизностных метрик.

### color_light
Модуль анализирует цвет и свет в кадрах: цветовую палитру, распределение оттенков, насыщенности и яркости, контраст, стабильность освещения. Он формирует компактные гистограммы, выделяет доминирующие цвета и оценивает lighting‑характеристики (направление, равномерность, «киношность» и т.д.). Эти фичи важны для оценки визуального стиля, атмосферы и используются в `shot_quality`, `frames_composition`, `uniqueness`.

### frames_composition
Модуль оценивает композицию кадров: расположение объектов и лиц, баланс, правило третей, глубину сцены и структурные паттерны. Он использует YOLO, FaceMesh и MiDaS, чтобы получать позиции ключевых объектов, лиц и карту глубины, после чего вычисляет метрики композиционного качества. Эти признаки отражают профессиональность постановки, читаемость кадра и визуальный комфорт.

### shot_quality
Модуль оценивает техническое и визуальное качество кадра/видео: резкость, шум, глубину, экспозицию, артефакты и эстетичность. Он использует MiDaS (глубина), DnCNN/CBDNet (шум/деблюринг) и AestheticPredictor (ResNet/CLIP‑базированный) для построения комплексного quality‑профиля. Эти фичи критичны для моделей ранжирования, фильтрации «плохого» контента и анализа продакшн‑уровня.

### micro_emotion
Модуль извлекает микроэмоции и Action Units (AU) с использованием OpenFace (через Docker), а затем агрегирует их в компактный набор признаков. Он строит детальные временные кривые AU, позы головы, взгляда и детектирует микровыражения и быстрые изменения эмоционального состояния. Эти сигналы дополняют `emotion_face`, давая более тонкий, быстрый эмоциональный слой для поведенческих и engagement‑моделей.

### cut_detection
Модуль детектирует монтажные переходы (hard cuts, простые переходы) и, при необходимости, их типы. Он сочетает SSIM, цветовые гистограммы, иногда CLIP/ResNet‑фичи и другие сигналы, чтобы надёжно находить границы шотов/сцен. Его результаты служат основой для `video_pacing`, `story_structure`, анализа структуры и ритма монтажа.

### video_pacing
Модуль измеряет темп и ритм видео: длины шотов, cut‑rate, story‑energy curve, motion‑based pace, цветовой и световой пейсинг. Он строит шот‑структуру, pace‑кривую, статистики шотов (mean/median/std/entropy), бурсты быстрых склеек и структурные скорости (intro/main/climax). Эти фичи используются для анализа динамичности, предсказания удержания и как вход для `uniqueness` и высокоуровневых моделей.

### story_structure
Модуль описывает нарративную структуру: сегментацию истории (по CLIP‑эмбеддингам), energy curve, кульминацию, hook и персонажей, а также текстовые топики (по субтитрам). Он считает story segments, narrative continuity, climax position/strength, story_energy_curve (в том числе downsampled для трансформера) и topic‑фичи. Это основной источник story‑level информации, который связывает визуальный, аудио и текстовый слои.

### text_scoring
Модуль анализирует on‑screen текст: синхронизацию текста с движением, лицами и аудио, CTA, длительность и динамику появления текста, layout и акценты. Он использует OCR (EasyOCR/pytesseract) плюс motion/face/audio сигналы, чтобы вычислять text_action_sync_score, CTA‑параметры, text_on_screen_continuity, text_switch_rate и text_emphasis peaks. Эти фичи важны для CTR, вовлечения и служат входом для `uniqueness` и моделей рекомендаций.

### high_level_semantic
Модуль строит high‑level семантику видео на основе CLIP‑эмбеддингов кадров/сегментов. Он получает frame‑level embeddings, агрегирует их в per‑video embedding, оценивает topic_probabilities и может извлекать семантические события. Его выходы являются центральным семантическим представлением, используемым в `similarity_metrics`, `uniqueness`, `story_structure` и рекомендационных моделях.

### similarity_metrics
Модуль вычисляет метрики схожести текущего видео с другими: cosine/L2 по embedding, корреляцию временных кривых, расстояния по гистограммам и Jaccard‑подобные меры по наборам объектов/тем. Он не запускает модели сам, а работает с эмбеддингами и фичами из других модулей, формируя `similarity_scores`. Эти метрики используются для поиска похожего контента, дедупликации и как важный сигнал для `uniqueness` и рекомендаций.

### uniqueness
Модуль агрегирует мульти‑модальные признаки (semantic, visual, editing/pacing, audio, text, behavioral, temporal) в метрики новизны. Он считает semantic_novelty (max/top‑K), topic и концепт diversity, визуальную/аудио/текстовую/ритмическую новизну, multimodal_novelty_score, novel_event_alignment_score и overall_novelty_index, а также трендовые показатели (trend_alignment, early_adopter). Это надстройка, оценивающая, насколько контент уникален относительно разных reference‑сеттов (глобальных, категорийных, исторических, авторских).


