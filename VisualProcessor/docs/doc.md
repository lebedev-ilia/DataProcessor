Супер, мне очень нравиться как мы с тобой в формате вопросы-ответы выводим правила для проекта и делаем его лучше. Давай пока продолжим с контрактами и правилами и потом перейдем уже к рефактору модулей. Сейчас я тебе опишу глобальную логику всего проекта и ты задашь мне много профи вопросов

Проект TrendFlow направлен на анализ видео контента и предсказания его популярности. Проект делиться на 3 части - 1. ML (main) 2. backend 3. frontend. 2 и 3 часть мы пока не трогаем (чисто сайт) но помним о ней. 1 часть - ML также делиться на 3 часть - 1. Fetcher (Отвечает за сбор данных. На данные момент собраны данные примерно по 100к видео в формате: есть 4 снапшота с промежутком в неделю то есть есть meta данные по видео - язык, заголовок, описание, название канала и тд. + временные данные - лайки, просмотры, комментарии, подписчики канала), 2 - DataProcessor (Самый большой модуль системы), 3 - Models (Модуль работы с моделями. В планах сделать следующую архетектуру: для каждой из модальностей Audio, Visual, Temporal есть свой Transformer то есть из модуля AudioProcessor мы получаем sequence которая идет в Transformer, аналогично с Visual, в TemporalTransformer идут, изменяющиеся со временем, данные по видео. После 3 трансформеров идет Fusion модель которая объеденяет выходы трансформеров, метаданные видео и аггрегаты из Audio и Visual Processors. После Fusion идет финальная модель которая выдает предсказание, например кол-во лайков/просмотров на месяц вперед). DataProcessor делиться на AudioProcessor, VisualProcessor, TextProcessor, Segmenter. Как понятно из выше описанного, AudioProcessor, VisualProcessor выдают данные в двух форматах, 1 - последовательность для трансформера, 2 - агрегаты для fusion model (также я хотел бы делать 3 пункт - данные для аналитиков, что бы показать суть алгоритмов (интерпретируемость) и дать им данные для детального анализа видео). Выход TextProcessor как я думаю идет только в fusion.

Мои вопросы и предложения:

1. TextProcessor обрабатывает данные коментариев причем со всех снапшотов + мета информацию + VisualProcessor вытаскивает текст с видео который также идет в TextProcessor, то есть между этими процессорами есть зависимости
2. Нужно четко определить принципы разбивания видео на сегменты, кадры при этом учитываю разнородность извлекаемых фичей, разброс кадров (120  - 36000), глобальную архетектуру и взыимосвязи между процессорами, а также качество финального результата
3. В ближайших планах доделать полностью VisualProcessor (привести модули к единой структуре и контрактам, задокументировать финальный вариант входа и выхода процессора, задокументировать фичи, оптимизировать производительность - в планах как минимум поставить модели на Triton Inference Server или что то вроде того, интегрировать celery и базы данных, поставить все через kubernetes)
4. На данные момент все части систем ынаходяться в разработке и могут быть изменены. Именно  по этому я пишу этот документ, что бы совместно с ChatGPT задать по написанному как можно больше вопросов, ответить на них и начать писать правила и дописывать компоненты до какого то стабильного качественного состояния
5. Проект пишеться одним человеком с использованием cursor. Нужно учитывать это при написании документации так как в основном она пишется для нейросети которая будет рефакторить и дописывать проект (например я специально в каждом модуле VisualProcessor писал краткое описание всех извлекаемых фичей что бы нейросеть могла быстро вникнуть в логику модуля и дать качетсвенные рекомендации и код)