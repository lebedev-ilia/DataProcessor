core_providers:
  # Minimal, fast smoke config to validate contracts end-to-end (Segmenter → VisualProcessor → NPZ/manifest).
  core_clip: true
  core_face_landmarks: true
  core_depth_midas: true
  core_object_detections: true
  core_optical_flow: true

modules:
  # Tier-0 modules smoke
  scene_classification: true
  face_detection: false
  detalize_face_modules: false
  emotion_face: false
  behavioral: false
  optical_flow: false
  action_recognition: false
  color_light: false
  frames_composition: false
  shot_quality: true
  cut_detection: true
  video_pacing: true
  story_structure: true
  micro_emotion: false
  similarity_metrics: true
  text_scoring: true
  uniqueness: true

global:
  root_path: null
  frames_dir: null
  rs_path: null
  max_parallel_modules: 1
  gpu_max_concurrent: "auto"

core_clip:
  model_name: "ViT-B/32"
  # Controlled by scheduler (DynamicBatching, in development). For now must be explicit.
  batch_size: 16
  sampling:
    target_frames: 120
    min_frames: 60
    max_frames: 200

core_depth_midas:
  runtime: "triton"
  triton_http_url: "http://triton:8000"
  triton_model_name: "midas_384"
  triton_model_version: null
  triton_input_name: "INPUT__0"
  triton_output_name: "OUTPUT__0"
  triton_datatype: "FP32"
  triton_preprocess_preset: "midas_384"
  out_width: 384
  out_height: 384
  # Controlled by scheduler (DynamicBatching, in development). For now must be explicit.
  batch_size: 16
  frames_bgr: false
  sampling:
    target_frames: 120
    min_frames: 60
    max_frames: 200

core_face_landmarks:
  venv_path: "/home/ilya/Рабочий стол/TrendFlowML/DataProcessor/VisualProcessor/core/model_process/core_face_landmarks/.core_face_landmarks_venv"
  use_pose: true
  use_hands: true
  use_face_mesh: true
  sampling:
    target_frames: 120
    min_frames: 60
    max_frames: 200

core_object_detections:
  model: "yolov8n.pt"
  # Controlled by scheduler (DynamicBatching, in development). For now must be explicit.
  batch_size: 16
  device: "auto"
  box_threshold: 0.6
  iou-threshold: 0.3
  sampling:
    target_frames: 120
    min_frames: 60
    max_frames: 200

core_optical_flow:
  runtime: "triton"
  triton_http_url: "http://triton:8000"
  triton_model_name: "raft_256"
  triton_model_version: null
  triton_input0_name: "INPUT0__0"
  triton_input1_name: "INPUT1__0"
  triton_output_name: "OUTPUT__0"
  triton_datatype: "FP32"
  triton_preprocess_preset: "raft_256"
  model_version: "unknown"
  weights_digest: "unknown"
  precision: "fp32"
  sampling:
    target_frames: 120
    min_frames: 60
    max_frames: 200

scene_classification:
  # Tier-0: deterministic (no CLIP semantics), but keep advanced heuristic features enabled
  enable_advanced_features: true
  use_clip_for_semantics: false
  sampling:
    target_frames: 120
    min_frames: 60
    max_frames: 200

cut_detection:
  device: "auto"
  use_clip: false
  use_deep_features: false
  sampling:
    target_frames: 120
    min_frames: 60
    max_frames: 200

shot_quality:
  device: "cuda"
  sampling:
    target_frames: 120
    min_frames: 60
    max_frames: 200

video_pacing:
  downscale_factor: 0.25
  sampling:
    target_frames: 120
    min_frames: 60
    max_frames: 200

story_structure:
  # Baseline uses only core_clip; args kept for compat but unused.
  clip_model: null
  sentence_model: null
  subtitles: null
  sampling:
    target_frames: 120
    min_frames: 60
    max_frames: 200

uniqueness:
  repeat_threshold: 0.97
  sampling:
    target_frames: 120
    min_frames: 60
    max_frames: 200

similarity_metrics:
  top_n: 10
  reference_embeddings_npz: null
  sampling:
    target_frames: 120
    min_frames: 60
    max_frames: 200

text_scoring:
  # OCR artifact is optional; module will produce a valid empty output if not found.
  ocr_npz: null
  use_face_data: false
  sampling:
    target_frames: 120
    min_frames: 60
    max_frames: 200


